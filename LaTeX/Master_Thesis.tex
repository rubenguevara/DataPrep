% \documentclass[14pt, a4paper]{book}
\documentclass[12pt, a4paper]{book}
\usepackage{setspace}
\setstretch{1.5}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in,marginpar=0.9in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, fancyhdr, color, comment, graphicx, environ, bm}
\usepackage{coffeestains}
%\usepackage{xcolor}
%\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage[USenglish]{uiomasterfp}
%\usepackage{indentfirst}
%\usepackage{simpler-wick}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{siunitx}
\usepackage{physics}
%\usepackage{polynom}
%\usepackage{verbatim}
\usepackage{tensor}
\usepackage{simpler-wick}
\usepackage{todonotes}
\usepackage{slashed}
% \usepackage{dcolumn}% Align table columns on decimal point
\usepackage{listings}
\renewcommand{\lstlistingname}{Algorithm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[backend = biber, style = vancouver]{biblatex}
\usepackage{biblatex}
\usepackage[printonlyused,withpage]{acronym}
\addbibresource{refs.bib}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    % linkcolor=black,
    filecolor=magenta,      
    urlcolor=cyan,      
    % urlcolor=black,
    citecolor=cyan,
    % citecolor=black,
    pdftitle={Model\_independent\_search\_for\_Dark\_Matter\_using\_Machine\_Learning},
    pdfpagemode=FullScreen,
    }

\usepackage{tikz}
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\usepackage{subfiles} 


\title{Model independent search for Dark Matter using Machine Learning
} \subtitle{
    In final states with dileptons and Missing Transverse Energy with the ATLAS detector at the LHC 
}
\author{Ruben Guevara}
\date{Spring 2023}

\begin{document}
\pagenumbering{roman}

\uiomasterfp[
colour = orange,
dept = {Department of Physics},
long,
program = {Physics: Nuclear and Particle Physics},
supervisors = {Professor Farid Ould-Saada \and Dr. Eirik Gramstad},
image = {gort}
]
\newpage
\section*{Acknowledgements}
Thank you everybody<3<3 \\
Most importantly I want to thank the coffee machine at the section for High Energy Particle Phyiscs for the incredible support in times of need, as the coffee was free. \coffeestainA{1}{0.85}{-25}{3cm}{1.3cm}

\newpage
\begin{center}
\section*{Abstract}
Something something
\end{center}


\newpage
\tableofcontents
\listoffigures
\listoftables

\chapter*{List of Acronyms}
\textbf{SM} Standard Model\\\\
\textbf{DM} Dark Matter\\\\
\textbf{LHC} Large Hadron Collider\\\\
\textbf{MET} Missing Transverse Energy\\\\
\textbf{MC} Monte Carlo\\\\
\textbf{ML} Machine Learning\\\\
\textbf{NN} Neural Network\\\\
\textbf{BDT} Boosted Decision Tree\\\\
\newpage
\pagenumbering{arabic}



\chapter{Introduction}\label{chap:intro}
\subfile{chapters/Introduction.tex}

\part{The theory behind modern particle physics / Background}
\chapter{The Standard Model of Particle Physics}\label{chap:SM}
\subfile{chapters/Part_I/1_The_Standard_Model_of_Particle_Physics.tex}

\chapter{Dark Matter}\label{chap:DM}
\subfile{chapters/Part_I/2_Dark_Matter.tex}

\chapter{Production, Detection and Analysis}\label{chap:CERN_method}
\subfile{chapters/Part_I/3_Production_Detection_and_Analysis.tex}

\chapter{Machine Learning}
\subfile{chapters/Part_I/4_Machine_Learning.tex}


\part{Implementation / Methods}


\chapter{Data Preparation}\label{chap:data_prep}
\subfile{chapters/Part_II/5_Data_Preparation.tex}

\chapter{Machine Learning}\label{chap:ML}
\subfile{chapters/Part_II/6_Machine_Learning.tex}

\part{Results}

\chapter{Network opimization}\label{chap:network_opt}
\subfile{chapters/Part_III/7_Machine_Learning.tex}
\subfile{chapters/Part_III/8_Model_dependent_approach.tex}
\subfile{chapters/Part_III/8_Results.tex}

\chapter{Conclusion}\label{chap:conclusion}
\subfile{chapters/Conclusion.tex}

\clearpage


\appendix

\subfile{chapters/DSIDs.tex}

\subfile{chapters/Distributions_CR.tex}

\subfile{chapters/Distributions_jets.tex}

\subfile{chapters/New_features.tex}

\subfile{chapters/Depth_of_30.tex}





\chapter*{Logbook}

We are dropping $\Delta\phi(l_1,l_2)$ and $\Delta\phi(l_c,E_T^{miss})$ due the poor agreement between MC and data. The first one is most likely due us not including fake leptons (and also for all non SFOS final states). \todo{I will remove this sentence, but its just so we know}The latter is a problem that PhD. Even is being haunted by. 
There is also the problem of missing variables. For this theis, as shown in Table \ref{tab:paddable_variables}, we will record events with up to three jets in the final state. As mentioned earlier, there isn't 
always three jets in the final state, to mediate this problem I chose to set the $p_T$ to zero for the missing jets and $m_{jj}$ to zero if there are less than two jets, this is something that is physically reasonable as it doesn't violate any conservation laws. 
More problematic however is the $\eta$ and $\phi$ when there aren't jets. To mediate this I've set the values to -999, which has no physical meaning and is impossible to achieve, this I did so it becomes easier for us to identify the jagged arrays further in the network preparation. 
The missing variables is not a problem when making BDTs with XGBoost. There is also another problem, albeit less problematic than the previous ones, with the final states that are not SFOS, as the MC generated background 
on these tend to be lower than the recorded data. The number of events that are not SFOS are minimal though, and we think the reason it doesn't fit the data is because we are not including fake leptons. \todo{something more to add?}

\subsection*{Zp Dark Matter dataset}
To train the networks I will utilize two methods\todo{see next comment for the two methods}. The first one being this where the dataset being sent into the ML network will contain every single DM MC sample available. 
So far there are 154 different MC samples, these are based on three theories. A Light Vector (LV), Dark Higgs (DH) and Effective Field Theory (EFT) which produces the WIMP DM particles, and a new theoretical particle, $Z'$, 
that decays into a lepton pair. The three theories are divided further into MC samples with a Light Dark Sector (LDS) and High Dark Sector (HDS) which tells us therange of the Dark Matter candidate mass. 
And lastly it is divided further into more MC samples with different masses for $Z'$. This dataset includes all of these samples such that the network learns Dark Matter in a model independent way.

\subsection*{"Ensemble" dataset / Model indepedence}\todo{This subsection and is outdated with our current plan, and this will change when we discuss further how to implement the model-independent aspect.}
Another approach is to make multiple datasets and combine the results of every network into a "big network". This is the second approach which I call ensemble modelling. The thought behind this is that when 
training a network using the full dataset it might only focus on a the more resonant models with fixed masses. Also, every different DM sample has different phenomenology, specially in the future when I will 
be receiving SUSY samples, meaning that it also might not train the network physics. Thus if we were to train a network one one sample at a time it would be the perfect scenario. 
However as will become apparent in Section \ref{sec:wgts}, the datasets (even the full DM dataset) are extremely unbalanced. To put some numbers, on each DM MC sample there are roughly 40,000 MC events, 
and for the SM background (with a massive MET > 50GeV cut!) there are roughly 87,000,000 MC events. Factoring the weights to re-weight the MC events to expected events gives us an extremely low statistics dataset, which punishes the network for guessing correctly. \\
\\\textit{\textbf{THIS IS OUTDATED AND WILL MOST LIKELY BE CHANGED TO THE SIGNAL REGION APPROACH, WHERE WE STATISTICALLY COMBINE WHAT THE NETWORKS LEARN IN A MODEL DEPENDENT WAY}} Thus making the approach to teach the networks one MC sample at a time is impossible. So far I have tried dividing the the MC samples into 18 different categories. First into their respective theory. 
Then into LDS or HDS. Then into three $m_{Z'}$ regions, where I've defined the \textit{low mass region} to be $\le 600$ GeV, the \textit{middle mass region} to be $>600$ $\cap\le 1100$ GeV and 
the\textit{ high mass region }to be $\ge 1100$ GeV. Using a NN with three hidden layers I get poor results, but changing this into one works! Will repeat with real weights, it didn't work. 
\subsection*{Interesting articles}
\begin{itemize}
	\item From "Search for a charged Higgs boson decaying into a top and a bottom quark at $\sqrt{s} = 13$ TeV" \cite{Benchekroun_2710015} they say (chap 4.2): "The signal samples are trained against all backgrounds, which are weighted according to their
	cross-sections."
	\item 
\end{itemize}


\clearpage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\end{document}
