\documentclass[12pt, a4paper]{book}
\begin{document}
\graphicspath{{../../figures/}}

Now that we have established the necessary theoretical groundwork of particle physics in Chapter \ref{chap:SM} and explored our DM candidates in \ref{chap:DM}, it is time to explore how this knowledge can be applied. This leads us to ask important questions such as, 
how can we measure what we have learned? How do we put it into practice? Most importantly, how can we use this knowledge to uncover new physics phenomena?\\
\\To answer these questions, we have divided this chapter into three sections, each of which will focus on a different aspect of experimental particle physics. The first section will delve into particle production, 
followed by an examination of particle detection with the ATLAS detector at the LHC, and finally, we will explore the intricacies of data analysis in particle physics.

\clearpage
\section{Particle production}
As we have already seen the shape of the SM we are now ready to dive into the subject of how we can produce the particles that we wish to detect. In this chapter we will start from the basic kinematics of particles 
and then move to more complex variables that will be of use when analyzing data from detectors. The material for the first section is based on Thomson's book Modern Particle Physics \cite{THOMSON}, Jackson's "Kinematics" \cite{Jackson_kin}
and Vadla's PhD. thesis \cite{KNUT_VADLA}.

\subsection{Particle kinematics}\label{sec:particle_kinematics}
When working on a relativistic setting, such as we do in high energy particle physics, four vectors are the natural object to consider to generally describe our particles. As we are mainly interested in the motion of the particles, 
we will look at the four-momentum. Instead of using general variables, we will describe the particles using the four-momentum in terms of the geometry of the detectors, that means we will use 
the polar angle, $\theta$, and the azimuthal angle, $\phi$, such that we have
\begin{equation}\label{eq:four-momentum}
    p^\mu = (E, p_x, p_y, p_z) \overset{Lab}{\longrightarrow} (E, p_T\cos\phi, p_T\sin\phi, \abs{\mathbf{p}}\cos\theta)
\end{equation}
where $p_T$ is the \textit{transverse momentum} expressed as
\begin{equation}\label{eq:transverse_momentum}
    p_T \equiv\sqrt{p_x^2 +p_y^2} = \abs{\mathbf{p}}\sin\theta
\end{equation}
Where the relativistic energy and momentum are given as, $E=\gamma\beta$ and $\mathbf{p}=\gamma m\bm\beta$, with $\gamma = 1/\sqrt{1-\beta^2}$ and $\bm\beta = \mathbf{v}/c$\footnote{As this is a particle physics thesis I will convert to Natural Units where we set $c=1$} 
where $m$ is the mass of the particle 
and $c$ is the speed of light in vacuum. By contracting\footnote{Using the particle physicists convention of the Minkowski metric tensor $\eta_{\mu\nu}$,  (+, -, -, -)} two four-momentum we get the important Lorentz invariant 
square of the \textit{invariant mass}
$$
    m^2 = p_\mu p^\mu = E^2 - \abs{\mathbf{p}}^2 
$$
which can be generalized for a system containing $n$ particles as
\begin{equation}\label{eq:invariant_mass}
    m^2 = p_\mu p^\mu = \left(\sum_{i=1}^n E_i\right)^2 - \left(\sum_{i=1}^n\mathbf{p}_i\right)^2
\end{equation}
As this thesis will focus on a dilepton (and missing transverse energy) final state, which is of the type $2\rightarrow2$ (+MET) then the invariant mass of the two leptons in the final state will be of interest, 
we will denote this as $m_{ll}$. From this we can also get another interesting variable, the \textit{transverse energy}. This follows directly from the same equation
\begin{equation}\label{eq:transverse_energy}
    E_T = \sqrt{m^2 + p_T^2}
\end{equation}
The invariant mass is what we measure in the final state only. But as we are going to use data\footnote{And mostly simulations mimicking the ATLAS detector} from the LHC, from which the initial state is controlled, 
it will be of interest to see what the total energy and momentum of the two protons are. The term for this is called the \textit{centre-of-mass} energy, $\sqrt s$, where $s$ is defined by the same formula in Eq. (\ref{eq:invariant_mass}), 
with the difference being that we look at the initial particles. For this thesis we will look at data and simulations of Run II from the LHC, which had $\sqrt s = 13$ TeV. \\
\\As this thesis aims to search for DM, which we know does not interact with matter in the same way as neutrinos, meaning it leaves no signal in detectors. 
As we know both the centre-of-mass energy, $\sqrt s$, and the invariant mass of all particles in the final state, Eq. (\ref{eq:invariant_mass}). Then the presence of the non-interacting particles can often be 
inferred from the presence of \textit{missing transverse energy}\footnote{Also called \textit{missing momentum}} (MET), which is defined by
\begin{equation}\label{eq:MET}
    E_T^{miss} = \mathbf{p}_{miss} \equiv -\sum_i \mathbf{p}_{T,i}
\end{equation}
where the sum extends over the measured momenta of all the observed particles in an event. From this formula, if all particles produced in the collision have been detected, then this sum should be zero. Meaning that 
significant MET is therefore indicative of the presence of an undetected particle. \\
\\Another useful kinematic variable is the \textit{hadronic activity} which is the scalar sum of the transverse momentum of all jets in an event, defined as
\begin{equation}\label{eq:HT}
    H_T = \sum_{i\in\{jets\}} \vert\vert \mathbf{p}_{T,i}\vert\vert
\end{equation}
this gives a measurement of the hadronic energy scale of an event. Another handy trick comes from the realization that the centre-of-mass frame is between the hadrons, where the total momentum is given as a function of the energy of the hadron. 
This means that the final state particles are boosted along the beam axis. With this realization we can now introduce a Lorentz invariant\footnote{Under boosts along the beam axis} kinematic property known as the \textit{rapidity, y} used to express the lepton angles
\begin{equation}\label{eq:rapidity}
    y \equiv \frac{1}{2}\ln\left(\frac{E+p_z}{E-p_z}\right)  
\end{equation} 
We can use that $p_Z = E\cos\theta$ in the high-energy limit as the mass is negligible. In this limit we can use the \textit{pseudorapidity}, $\eta$, defined by
\begin{equation}\label{eq:pseudorapidity}
    \eta \equiv -\ln\left(\tan\frac{\theta}{2}\right)
\end{equation}
The pseudorapidity is an interesting variable as it can tell us how close to the beam the final state particles are, where the higher $\vert\eta\vert$ means closer to the beam. This variable can also be negative, meaning backwards scattering. 
From this we can define a new variable which will come handy with particle identification, which is called the \textit{R-cone}, that defines a circle in $(\eta,\phi)$-space surrounding the object of interest. It is defined as
\begin{equation}\label{eq:R-cone}
    \Delta R = \sqrt{(\Delta\eta)^2+(\Delta\phi)^2}
\end{equation}
Another interesting variable is the \textit{transverse mass}, defined as
\begin{equation}\label{eq:transverse_mass}
    m_T^2 = m^2 + p_T^2
\end{equation}
where $m^2$ is the invariant mass defined in Eq. (\ref{eq:invariant_mass}). What is interesting with this variable is that it is the equivalent of the invariant mass equation, that takes into account invisible particles. 
We can take this further by looking at a variable which calculates a transverse mass for two leptons by distributing the total $p_T^{miss}$ among the two systems, 
and minimizing the maximum of the two transverse masses by varying the distribution of the $p_T^{miss}-$vector in terms of the size of $p_T$. This is called the \textit{stransverse mass} and is defined by
\begin{equation}\label{eq:stransverse_mass}
    m_{T2}^2(\chi) = \underset{\slashed{\mathbf{q}}^{(1)}_T + \slashed{\mathbf{q}}^{(2)}_T = \slashed{\mathbf{p}}_T}{\min}
    \left[\max \left\{m_T^2\left(\mathbf{p}_T^{\ell_1}, \slashed{\mathbf{q}}^{(1)}_T;\chi\right), m_T^2\left(\mathbf{p}_T^{\ell_2}, \slashed{\mathbf{q}}^{(2)}_T;\chi\right) 
    \right\}\right] 
\end{equation}
where $\slashed{\mathbf{q}}_T$ are "dummy 2-vectors", $\chi$ is a free parameter used to "guess" the mass of the invisible particle, and $m_T^2\left(\mathbf{p}_T, \mathbf{q}_T\right)$ is an application of 
Eq. (\ref{eq:transverse_mass}) using two particles:
$$
m_T^2 \left(\mathbf{p}_T, \mathbf{q}_T\right) = 2(p_T q_T - \mathbf{p}_T\cdot\mathbf{q}_T)
$$
For a more detailed explanation and interpretation of the stransverse mass we refer the reader to the paper by Barr et al. \cite{Barr_2003}. Even though the stransverse mass was made with neutralinos in mind, it can still 
be used to calculate SM processes. For example, if we want to reduce $WW$ background events, we can first recall that each boson can decay as $W\rightarrow l+\nu_l$ with the $W$ mass as an endpoint. 
Meaning that we can use $m_{T2}$ to reduce the $WW$ events in a dilepton final state by requiring that $m_{T2} > m_W$.

\subsection{Proton-proton collisions}
With all the kinematics out of the way the question of how the particles are produced still remains. The answer could be an electron positron collider, as they did in LEP, a proton anti-proton collider, like the Sp$\overline{\mbox{p}}$S. As this thesis uses LHC data we will look at proton-proton collisions.
The reason as to why this works is that protons are also made of elementary particles, two \textit{up} and one \textit{down} to be specific.
Because of this it is not hard to realize that the Feynman rules acquired from the SM (Chapter \ref{chap:SM}) also apply here. In this subsection we will study how different effects of $pp-$collisions affect 
the cross-section, and therefore the expected number of events to occur, from a kinematical point of view.

\subsubsection{Parton Distribution Functions}
Although the proton is made up for two up quarks and one down quark, called the \textit{valence quarks}. The proton also consists of gluons and other quarks, called partons. These partons become important in deep inelastic scatterings, 
where the proton breaks apart due the high energies in the collisions. As we accelerate the protons before colliding them, we also accelerate the quarks and gluons inside it, each of the quarks carry a momentum fraction $x$, 
called the Bjorken $x$. We can then calculate the invariant mass of two colliding partons $q_1$ and $q_2$, with the momentum fraction $x_1$ and $x_2$, from the proton's momentum $p_1$ and $p_2$ respectively as
$$
m^2=x_1x_2s
$$
where $s$ is the centre-of-mass energy squared of the $pp-$system. The valence quarks in the proton do not only interact with the other valence quarks in the other proton, but they might also emit gluons which decay into quark anti-quarks pairs, 
making a “sea” of gluons and quarks around the valence quarks. The momentum of the partons inside the proton are dependent on the momentum transfer $Q^2$ and is represented by an experimentally determined momentum distribution, 
known as the \textit{parton distribution function} (PDF) \cite{PDF} $f(x,Q^2)$. \textbf{Question: Should I say more about PDFs than this? I feel like it is not relevant for my thesis, but is still good to know.} In other words, the PDFs give the probability of a parton to collide with the momentum fraction $x$. The shape and form the PDFs play 
an important role of estimating the processes that occur after the proton collisions, and therefore are crucial when simulating events using Monte Carlo \cite{MC_PDF}.  If we take as an example the process 
$pp\rightarrow l^+l^-+X$ where $X$ denotes any hadrons formed by the remaining quarks. Figure \ref{fig:Feynann_PDF} showcases the process.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{PP_Feynmann_Diagram.png}
    \caption[Feynman diagram from $pp-$collision]{Feynman diagram depicting the $pp\rightarrow l^+l^-+X$ process where $X$ denotes any hadrons formed by the remaining quarks
    not taking part in the $q\bar{q}-$collisionc carrying $x_1$ and $x_2$ momentum from the protons $p_1$ and $p_2$ respectively}\label{fig:Feynann_PDF}
\end{figure}
\\The cross-section for the process is 
$$
\sigma\left(p(p_1) p(p_2)\rightarrow l^+l^- +X\right)=
$$
\begin{equation}\label{eq:PDF_xsec}
    \int_{0}^{1}\int_{0}^{1}\sum_{f}f_f(x_1)f_{\bar{f}}(x_2)\cdot\sigma\left(q_f(x_1p_1)+\bar{q}_f(x_2p_2)\rightarrow l^+l^- \right) dx_2dx_1
\end{equation}
where $\sigma\left(q_f(x_1p_1)+\bar{q}_f(x_2p_2)\rightarrow l^+l^- \right)$ is calculated using the rules explained in Chapter \ref{chap:SM}, and $f$ are the PDFs. As we can see this can drastically change the cross-section, and with that the 
probability, for an event we want to study. 

\subsubsection{Breit-Wigner resonance}
Another important aspect when looking at particle collisions is the \textit{Breit-Wigner resonance}. All unstable particles have a decay rate, $\Gamma$, (given as inverse of the lifetime $\tau$) which is present in the wave function, 
$\Psi \propto e^{-i(m-i\Gamma/2)}$. This decay rate also becomes apparent when the unstable particle is the propagator of an event we are studying 
\begin{equation}\label{eq:Breit-Wigner}
    \sigma \propto\frac{1}{(s-m^2)^2-m^2\Gamma^2}
\end{equation}
From this we can see that as the square of the centre-of-mass energy $s$, approaches the unstable particles mass $m$, there will be a resonance at the invariant mass of the unstable particle, this is called the Breit-Wigner resonance. 
It is because of resonances like this that we can identify particles such as the $Z$ boson. This resonance will also be present when studying some BSM DM models in this thesis.\\
\textbf{ Question: should I cite the original paper or Z discovery?}


\subsubsection{Expected events}
The most important value we need to know when studying $pp-$collisions is the number of interactions taking place, this is defined as
\begin{equation}\label{eq:expected_events}
    N=\sigma\int\mathcal{L}(t)dt, \qquad\text{where}\quad \mathcal{L}=f\frac{n_1n_2}{\sigma_x\sigma_y}
\end{equation}
where $\sigma$ is the cross-section of the interaction as expressed in Eq \ref{eq:cross_sec} while also taking into account the effects from Eq. (\ref{eq:PDF_xsec}) and Eq. (\ref{eq:Breit-Wigner}).
The last three symbols come from accelerator kinematics where $\sigma_{x,y}$ denotes the beam size, $f$ is the frequency of bunch crossings, and $n_{1,2}$ is the number of particles in bunches.





\clearpage
\section{The ATLAS detector}\label{sec:ATLAS}
We have so far on this chapter discussed how particles are produced. But we so far not yet explained how we actually detect them, arguably the most important matter in the field of high energy particle physics.
This section of the chapter is just about that, and we will explain how the detection happens in A Toroidal LHC ApparatuS, or more commonly know as the ATLAS detector. Figure \ref{fig:ATLAS_detector} showcases 
the detector and its size. The information of this section is largely based on the original ATLAS Collaboration introduction paper presented to CERN 
\cite{Aad:1129811}.\\
\\The ATLAS detector is a general multipurpose\footnote{Probing $pp-$ and $AA-$ (heavy ions) collisions.} detector located at the LHC and covers nearly the entire solid angle 
around the collision point, as described with the $(\eta,\phi)-$coordinates in Section \ref{sec:particle_kinematics}. The ATLAS detector consists of four main subdetectors; ($i$) an inner tracking detector (ID), 
an ($ii$) electromagnetic calorimeter (ECAL), and a ($iii$) hadronic calorimeter (HCAL), and lastly ($iv$) a muon spectrometer (MS). Figure \ref{fig:ATLAS_layers} visualizes the four $(i)-(iv)$ main sub-detectors, 
along with how the different particle types interact with each layer. A brief description of each layer is explained in this section.
\begin{figure}[!ht]
	\centering
    \includegraphics[width=1\textwidth]{ATLAS_detector.jpg}
    \caption[The ATLAS detector]{Cut-away view of the ATLAS detector, image taken from Ref. \cite{Aad:1129811}}\label{fig:ATLAS_detector}
\end{figure}
\begin{figure}[!ht]
	\centering
    \includegraphics[width=0.80\textwidth]{ATLAS_detection.jpg}
    \caption[Illustration of the ATLAS detector layers]{Illustration of the ATLAS detector layers, image taken from Ref. \cite{Pequenao:1505342}}\label{fig:ATLAS_layers}
\end{figure}

\subsection{Inner detector}\label{sec:ID} 
The inner-detector (ID) system is immersed in a 2T axial magnetic field and provides charged-particle tracking in the range $\abs{\eta}<2.5$. The ID provides the first measurements of the 
momentum and identification of electrically charged particles, as these can be determined by the curvature of their reconstructed tracks. 
The ID is made of three independent systems; the pixel detector, the semiconductor tracker (SCT) and 
the transition radiation tracker (TRT). \\
\\The pixel detector is made up of 80 million silicon pixel sensors, each of size $50\times400\mu$m$^2$, %with a resolution of $14\times155\mu$m$^2$, 
and spread over multiple layers. Outside the pixel layers 
are the SCTs, which consist of silicon microstrips trackers, also placed on multiple layers. The SCT covers the pseudorapidity region $\abs{\eta}<2.5$. Furthest away from the interaction point lies the TRTs, 
they consist of 4 mm in diameter straw tubes, which enables track-following up to $\abs{\eta}=2.0$ 

\subsection{The calorimeters}\label{sec:calories}
The ATLAS detector has two types of calorimeters, the \textit{electromagnetic calorimeter} (ECAL) and the \textit{hadronic calorimeter} (HCAL), both designed to fully stop certain types of particles. 
Both calorimeters covers a pseudorapidity range $\abs{\eta}<4.9$. The ECAL is immediately surrounding the inner detector and is divided into a barrel part ($\abs{\eta}<1.475$) and two end-cap components ($1.375<\abs{\eta}<3.2$). 
The ECAL consists of absorbing lead plates, with liquid Argon (LAr) in between. The thickness of the calorimeter is made to fully measure the shower of photons and electron/positrons. The muons will only lose 
a small fraction of their energy as they have longer interaction lengths with lead.\\
\\The HCAL is immediately surrounding the ECAL on all sides and consists of two types of detectors. In the barrel ($\abs{\eta}<1.0$) and the extended barrel regions ($0.8<\abs{\eta}<1.7$), the HCAL is made of 
steel plates with plastic scintillator tiles as active material. While on the end-cap regions ($1.5<\abs{\eta}<3.2$) there are hadronic LAr detectors, with absorbing copper plates as active material; in the 
forward region ($3.1<\abs{\eta}<4.9$) a combination of copper and tungsten plates are used as active material. The active materials are chosen to maximize the interaction cross-section with hadrons, such as neutrons, 
protons and pions. The depth of the HCALs is also designed to fully stop the particles and their showers in order to measure their total energies. Hadrons are efficiently stopped at the HCALs, meaning that only 
muons and invisible particles, such as neutrinos and potentially dark matter, leave the HCAL.

\subsection{Muon spectrometer}\label{sec:MS}
The outermost layer of the ATLAS detector is the \textit{muon spectrometer} (MS), dedicated to the measurement of the muons momenta. The MS, similar to the ID, consists of multiple layers of detector material, 
and is immersed in a strong magnetic field to bend the trajectories of the charged muons. The MS is made of four different types of detector component: $(i)$ Monitored Drift Tubes (MDTs) on the barrel, 
$(ii)$ Cathode Strip Chambers (CSCs) dealing with the events closer to the beam line in the end cap, $(iii)$ Resistive Plate Chambers (RPCs) in the barrel and ($iv$) Thin Gap Chambers (TGCs) in the end caps. 
The MDTs and CSCs are used for tracking while the RPCs and TGCs are used for triggers. The tracking is provided for pseudorapidities up to $\abs{\eta}<2.7$, and the trigger system only extends to $\abs{\eta}<2.4$.

\subsection{Summary of ATLAS}
\textbf{Question: should I remove this summary or maybe combine it with the summary on Section 4.4?}\\
The main four parts of the ATLAS detector; \textit{inner-detector} (ID), \textit{electromagnetic calorimeter} (ECAL), \textit{hadronic calorimeter} (HCAL), and the \textit{muon spectrometer} (MS), have 
briefly been explained. For an easier understanding on the different parts I refer the reader to Figure \ref{fig:ATLAS_layers} which shows visually how different particles interact with the detector. We can start by noting that there are only three types 
of particles that interact with the ID, muons, protons and electrons (there might also be other electrically charged hadrons that interact with the ID). From the ID pixel and SCT detectors we can get information about $\phi,\eta, p_T$ and from the TRT we  
can get to know the \textit{charge} of the particles (depending on how the magnet bends their trajectories)\footnote{So as we know that the proton is positively charged, then we know that the muon and electron in Figure \ref{fig:ATLAS_layers} have negative charge}. 
Going to the next layer we see that in the ECAL the electron is completely stopped, such that we can know its energy, $E$. We can also note that the photons will interact with matter here and also be completely stopped. It is worth noting that in just the two 
first detector parts we have all the information we need to calculate interesting kinematic variables for the electron and photons. Going further into the detector, we see that the HCAL completely stops the proton and that now the detector interacts with the neutrons, giving us 
information about their energies (and location for neutrons). Lastly we see that the muon flies through all of this and finally hits the MS where we can get information about its energy. \textit{\textbf{Note/Question to Farid/Eirik: Why does it not stop?}}. \\
\\The noteworthy particle in Figure \ref{fig:ATLAS_layers}, which was not mentioned in the summary above either, is the \textit{neutrino}. The reason for this is that the neutrino, just like DM, is an \textit{invisible particle}, meaning that it does not 
interact with matter. We can still reconstruct information about the neutrino and DM however, we do this as explained in Section \ref{sec:particle_kinematics} with variables such as the MET, Eq. (\ref{eq:MET}), as we know the centre-of-mass energy that went 
into the collision. \\
\\Another thing to add is that the explanation of the detector is brief in this thesis. The reality is much more complicated than this as there are, i.e. \textit{triggers} (which might be excluding DM events) choosing which events are 
"important enough to record" as the \textit{hardware and data storage} is a big problem considering the magnitude of events that happen each second. There are as well as algorithmic procedures to calculate the variables aforementioned from the detector signal 
which both consume energy and time. As this is not the focus of my thesis I will refer the reader to further literature explaining the real hardships when collecting data at the ATLAS detector and other LHC projects. \cite{Trigger_1}\\
\textit{\textbf{Question: Is it okay for me to do this and move on? Or should I write about this?}}





\clearpage
\section{Data analysis}\label{sec:data_anal}
The time has come to explore how we can search for new physics phenomena, now that it has been established how particles are produced and how we detect them. In this section we will take into account the classic way of searching for new physics which is called 
the \textit{cut and count method}, but there are other methods to search for new physics, such as Machine Learning (ML) which is the method pursued in this thesis. There might be other methods, such as 
Quantum ML, but as of today we are still in an early stage of the technology \cite{QML}\todo{Can I include this?}. To give a short description of the cut and count method, it makes kinematical \textit{cuts} such that we isolate signal from background. The way 
the signal and background are made is by Monte Carlo (MC) simulations, such that we can more easily identify SM background from DM and then compare our results with real data. As it wouldn't be a real experimental physics discovery without making a statistical 
analysis of the results we will also explain how we utilize this tool. To guide us through this process I will use $ZZ^{(*)}$ channel in the discovery of the Higgs Boson in 2012 \cite{Higgs_discovery_2012} as an example of the success of this method.

\subsection{Cut and count}
The cut and count method is what is currently the standard method of doing data analysis with CERN related research\footnote{This standard method is shifting towards machine learning}. As the name implies, the cut and count method works by making cuts on kinematical variables and afterwards counting how many events are left. 
The goal of using this method is to make cuts such that we remove as many background events as possible while also keeping as many signal events as possible. For example, if we were to study a new physics model with a new light vector boson behaving similarly to the $Z$ boson, but with a higher mass, 
then a good kinematical cut to remove many background processes would be to require that $m_{ll}>100$ GeV, as this would remove the majority of $Z$-resonance from the final state, making it easier to "find" the new physics model. \\
\\To more thoroughly explore the cut and count method we can look at the Higgs discovery, in particular the $H\rightarrow ZZ^{(*)} \rightarrow 4l$\footnote{Where $l$ is for lepton, but only means $e^\pm$ or $\mu^\pm$. \textit{Question: should i explain why not $\tau$?}} channel. 
In section 4.1 of the original article \cite{Higgs_discovery_2012} there is something called \textit{event selection}. The event selection are the kinematical cuts used. The kinematical cuts used in the article, aside from the \textit{standard selection criteria} 
(see Chapter \ref{sec:obj_sel}), were:
\begin{itemize}
    \item Single-lepton or dilepton triggers
    \item Four leptons on final state with $p_T >20,15,10,7$ GeV, in the order of most energetic to least
    \item Higgs-boson candidates are formed by selecting two same flavor opposite charge lepton pairs
\end{itemize}
The first "cut" is to make sure that we have leptons on both the data and simulations. The second cut is that there needs to be four leptons, with different $p_T$ cuts to remove background while keeping as much signal as possible. And lastly we want to have two lepton pairs of the same-flavor 
with opposite-charge, this is to make sure that the leptons that we observe actually decay from $Z$-bosons. What now remains is to explore the "count" part of this method. When counting the events that pass the event selection one usually counts the background events that pass, the data 
points that pass, and also the signal events that pass. For the 2012 Higgs discovery, the Higgs channel with a Higgs mass of 125 GeV was used as signal. 
The results of the 2012 discovery is shown Figure \ref{fig:Higgs_ZZ}.\\
\begin{figure}[!ht]
	\centering
    \includegraphics[width=0.6\textwidth]{Higgs_ZZ_discovery.png}
    \caption[The Higgs discovery on the $ZZ^{(*)}$ channel]{The Higgs discovery on the $ZZ^{(*)}$ channel, image taken from Ref. \cite{Higgs_discovery_2012}}\label{fig:Higgs_ZZ}
\end{figure}
\\Although this section made the process look simple, it was through the effort of many scientists working together that made this happen. The great computational power needed to \textit{correctly} simulate events and reconstruct objects from detector signal was, and still is a big challenge.
Not to mention the state-of-the-art technology to be able to be to both accelerate the protons to an energy high enough to "create" new physics, and to actually be able to detect it. But this alone was not enough to claim the discovery, as we're physicist we need 
to be completely sure that what we show is true. We do this with statistics which is the next section.
\clearpage
\subsection{Statistical analysis}\label{sec:stat_anal}
To make any sort of claim in modern physics we should be absolutely certain that what we are claiming is true, as just making the cuts and isolating a signal to background is not enough. To be specific we need to be \textit{at least $5\sigma$ sure} to claim any new discoveries. 
But as not everything in experimental high energy particle physics is a new discovery, but is in fact for the most part \textit{exclusions}. Then we need to explain what an exclusion is, and how to make them. \\ 
\\This thesis is following Bugge's lecture notes on statistics [\textbf{Question: how do I cite his powerpoint from FYS5555}] and the example set by the ATLAS Collaboration article "Search for new particles in events with one lepton and missing transverse momentum in pp collisions at $\sqrt s$ = 8 TeV with the ATLAS detector" \cite{Stat},
where a Bayesian analysis is performed to set limits on the studied processes. Using the signal+background hypothesis, the expected number of events in each channel of a process we are studying is 
$$
    N_{\text{exp}} = \varepsilon_{\text{sig}}L_{\text{int}}\sigma B + N_{\text{bkg}}
$$
where $L_{\text{int}}$ is the integrated luminosity, $\varepsilon_{\text{sig}}$ is the signal selection efficiency defined as the fraction of signal events that satisfy the event selection criteria, $N_{\text{bkg}}$ is the expected number of background events, and $\sigma B$ is the 
cross-section times branching ratio of the process. Using Poisson statistics, the likelihood to observe $N_{\text{obs}}$ events is
\begin{equation}\label{eq:observed_events_prob}
    \mathcal{L}(N_{\text{obs}}\vert \sigma B) = \frac{(N_{\text{exp}})^{N_{\text{obs}}}e^{-N_{\text{exp}}}}{N_{\text{obs}}!}
\end{equation}
We include uncertainties by introducing nuisance parameters $\theta_i$, each with a probability density function $g_i(\theta_i)$, and integrating the product of the Poisson likelihood with the probability density function. The integrated likelihood is
\begin{equation}\label{eq:observed_events_nuisance}
    \mathcal{L}_B(N_{\text{obs}}\vert \sigma B)=\int\mathcal{L}(N_{\text{obs}}\vert \sigma B)\prod g_i(\theta_i)d\theta_i
\end{equation}
where a log-normal distribution is used for the $g_i(\theta_i)$. The nuisance parameters are taken to be: $L_{\text{int}}, \varepsilon_{\text{sig}}$ and $N_{\text{bkg}}$, with the appropriate correlation accounted for between the first and the third parameters. The measurements of 
the two decay channels (muon or electron final state for $H\rightarrow lll'l'$) are combined assuming the same branching fraction for each, thus Eq. \ref{eq:observed_events_nuisance} remains valid with the Poisson likelihood replaced by the product of the Poisson likelihoods for the two channels. 
The integrated luminosities for the electron and muon channels are fully correlated. We can further use Bayes' theorem which gives the posterior probability that the signal has signal strength $\sigma B$:
\begin{equation}\label{eq:prior_prob}
    P_{\text{post}}(\sigma B\vert N_{\text{obs}}) = N \mathcal{L}_B (N_{\text{obs}}\vert\sigma B) P_{\text{prior}}(\sigma B)
\end{equation}
where $P_{\text{prior}}(\sigma B)$ is the assumed prior probability, here chosen to be flat in $\sigma B$, for $\sigma B$ > 0. The constant factor $N$ normalizes the total probability to one. The posterior probability is evaluated for each mass and decay channel as well as for their 
combination, and then used to set a limit on $\sigma B$.\\
\\As we can see, the inputs for the evaluation of $\mathcal{L}_B$ (and $P_{\text{post}}$) are $\varepsilon_{\text{sig}}$, $L_{\text{int}}$, $N_{\text{bkg}}$ and $N_{\text{obs}}$ and the uncertainties of the first three. The uncertainties for these should account for experimental 
and theoretical systematic effects as well as the statistics of the simulated samples. For this thesis the systematic uncertainties will not be calculated, but will rather be assumed to be $\pm$ 30\% of the background, as this is roughly the standard value of ATLAS papers.\\
\\To make exclusions we can use Eq. (\ref{eq:prior_prob}) to establish a \textit{confidence limit} (CL). CLs are defined as the probability to observe the number of events observed in an experiment, $N_{\text{obs}}$, or \textit{less} given signal+background. We usually define a signal+background 
hypothesis to be excluded when CL$_{s+b}$ < 5\%. Meaning a 95\% CL. Such that the probability to falsely exclude an existing signal(+background) is 5\%. The standard practice in particle physics when using CL is to \textit{set limits} on theoretical models, rather than exclude them.\\
\\On the other side, to claim any discovery in particle physics we need to know the \textit{significance} of any statistical fluctuation. Before getting to the significance we can discuss the \textit{p-value}, defined as the probability to observe the number of events observed in the experiment, $n_{\text{obs}}$, 
or \textit{more} given only background
\begin{equation}\label{eq:p-value}
    p = P(N\ge N_{\text{obs}}\vert \lambda = N_{\text{bkg}}) = \sum_{k=N_{\text{obs}}}^{\infty}\mathcal{L}(k\vert N_{\text{bkg}})
\end{equation}
The smaller the $p-$value, the less compatible an observation is with the background only hypothesis, meaning more likely to be a discovery. From this we can find the significance $Z$ by
$$
    p = \int_{-\infty}^{-Z} \frac{e^{-x^2/2}}{\sqrt{2\pi}}dx
$$
As mentioned in the start of this subsection, a discovery in particle physics is defined to be at least a $Z=5\sigma$ deviation from the background hypothesis, meaning that we would have a $p-$value of $p\le2.87\times10^{-7}$. In other words, with a $5\sigma$ deviation, the probability to 
falsely discover something is at worst one in roughly 3.5 million.\\ 
\\As the significance is an interesting quantity we can give it its own definition. In this thesis we will use the low statistics formula for the significance, as this is the most general one. We can either define the significance as the \textit{observed significance} by
\begin{equation}\label{eq:obs_sig}
    Z = \sqrt{2\left[N_{\text{obs}}\ln\frac{N_{\text{obs}}}{N_{\text{bkg}}}-N_{\text{obs}}+N_{\text{bkg}}\right]}
\end{equation}
or as the \textit{expected significance} by changing $N_{\text{obs}} \rightarrow N_{\text{sig}}+N_{\text{bkg}}$, where $N_{\text{sig}}$ is the number of signal events
\begin{equation}\label{eq:exp_sig}
    Z = \sqrt{2\left[(N_{\text{sig}}+N_{\text{bkg}})\ln\left(1+\frac{N_{\text{sig}}}{N_{\text{bkg}}}\right)-N_{\text{sig}}\right]}
\end{equation}
However, Eq. (\ref{eq:p-value}) did not include any nuisance parameters, it used Eq. (\ref{eq:observed_events_prob}) instead of Eq. (\ref{eq:observed_events_nuisance}). We want to express the significance with uncertainties. From "Discovery sensitivity for a counting experiment with background uncertainty" from Glen Cowan
\cite{Cowan_Uncertainty_in_sig}, we can use Eq. (17) on his paper that reads
$$
Z = \left[-2\left(N_{\text{obs}}\ln\left[\frac{N_{\text{obs}}+m}{(1+\tau)N_{\text{obs}}}\right] + m\ln\left[\frac{\tau(N_{\text{obs}}+m)}{(1+\tau)m}\right]\right)\right]^{1/2}
$$
where $m=\tau N_{\text{bkg}}$ and Eq. (19) that reads
$$
\tau=\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2}
$$
where $\sigma_{\text{bkg}}$ is the uncertainty of the background. This gives us
\begin{equation}\label{eq:significance}
    Z = \sqrt{-2\left(N_{\text{obs}}\ln\left[\frac{N_{\text{obs}}+\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} }{(1+\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2})N_{\text{obs}}}\right] + \frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} \ln\left[\frac{\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2}(N_{\text{obs}}+\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} )}{(1+\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2})\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2}}\right]\right)}
\end{equation}
Which makes for a better estimate of the significance one has in reality. \textit{\textbf{ Question: Should I add plots showing CLs and significance?}}\\
% \\The\todo{is this unnecesary} results CL limits results from the 2012 Higgs discovery \cite{Higgs_discovery_2012} can be seen in Figure \ref{fig:HIGGS_CL}, where we can see that there is a statistical fluctuation of the observed data compared to the background with $m_H=125$ GeV
% \begin{figure}[!ht]
% 	\centering
%     \includegraphics[width=0.7\textwidth]{CL_Higgs.jpg}
%     \caption[Confidence Limit on the Higgs discovery]{Confidence limit on the 2012 Higgs discovery, excerpt taken from Ref. \cite{Higgs_discovery_2012}}\label{fig:HIGGS_CL}
% \end{figure}

\clearpage
\section{Summary}
In this chapter we have studied how one can calculate the number of expected events from a $pp-$collision can lead to the discovery of new particles. Using special relativity as a tool, we can express the four-momentum of the particles with 
detector-coordinates, $p^\mu=(E,p_T\cos\phi,p_T\sin\phi,\abs{p}\cos\theta)$. From this four-momentum vector we can thereafter calculate interesting kinematic variables such as the invariant mass $m_{ll}$, missing transverse energy $E_T^{miss}$, 
stransverse momentum, $m_{T2}$, among others. Using the advanced ATLAS detector the four-momentum can be recorded from the accelerated protons at the LHC. With the recorded data and MC simulations taking into account the experimental features such as 
the PDFs and Breit-Wigner resonance, as well as the ATLAS kinematics, we can compare how the simulated events fare with the data recorded. By playing around with the kinematical variables of the particles and making cuts to isolate new physics signal 
we can see if there is a discrepancy between the data recorded and the SM background. After creating this signal region with the cut and count method we conduct a statistical analysis to see how the new theory/observed data deviates from our 
current understanding of physics.\\
\\This state-of-the-art method is what currently is being used at CERN and has lead to a great advancement in the field. However, with the rise of new technologies, such as \textit{machine learning}, which excel at classification tasks, a door has been 
opened to try new methods. In this thesis we will use ML to hopefully create a better and more general signal region than what the current cut and count method does. Before describing how, we will explain what machine learning is, this is the subject 
of the next chapter.\\
\\\textbf{ Question: I don't know if this summary is too dense, or if the opening to ML is too superficial. Feel free to comment as harshly as you want here :-)}

\end{document}