\documentclass[12pt, a4paper]{book}
\begin{document}
\graphicspath{{../../figures/}}

Now that we have established the necessary theoretical groundwork of particle physics in Chapter \ref{chap:SM} and explored our DM candidates in \ref{chap:DM}, it is time to explore how this knowledge can be applied. This leads us to ask important questions such as, 
how can we measure what we have learned? How do we put it into practice? Most importantly, how can we use this knowledge to uncover new physics phenomena?\\
\\To answer these questions, we have divided this chapter into three sections, each of which will focus on a different aspect of experimental particle physics. The first section will delve into particle production, 
followed by an examination of particle detection with the ATLAS detector at the LHC, and finally, we will explore the intricacies of data analysis in particle physics.

\clearpage
\section{Particle production}
Having already been introduced to the SM we are now ready to dive into the subject of how we can produce the particles that we wish to detect. In this chapter we will start from the basic kinematics of particles 
and then move to more complex variables that will be of use when analyzing data from detectors. The material for the first section is based on Thomson's book Modern Particle Physics \cite{THOMSON}, Jacksons' "Kinematics" \cite{Jackson_kin}
and Vadla's PhD. thesis \cite{KNUT_VADLA}.

\subsection{Particle kinematics}\label{sec:particle_kinematics}
When working on a relativistic setting, such as we do in high energy particle physics, four vectors are the natural object to consider to generally describe our particles. As we are mainly interested in the motion of the particles, 
we will look at the four-momentum. Instead of using general variables, we will describe the particles using the four-momentum in terms of the geometry of the detectors (See Section \ref{sec:ATLAS}), that means we will use 
the polar angle, $\theta$, and the azimuthal angle, $\phi$, such that we have
\begin{equation}\label{eq:four-momentum}
    p^\mu = (E, p_x, p_y, p_z) \overset{Lab}{\longrightarrow} (E, p_T\cos\phi, p_T\sin\phi, \abs{\mathbf{p}}\cos\theta)
\end{equation}
where $p_T$ is the \textit{transverse momentum} expressed as
\begin{equation}\label{eq:transverse_momentum}
    p_T \equiv\sqrt{p_x^2 +p_y^2} = \abs{\mathbf{p}}\sin\theta
\end{equation}
Where the relativistic energy and momentum are given as, $E=\gamma m$ and $\mathbf{p}=\gamma m\bm\beta$, with $\gamma = 1/\sqrt{1-\beta^2}$ and $\bm\beta = \mathbf{v}/c$\footnote{As this is a particle physics thesis I will convert to Natural Units where we set $c=1$, $\hbar = 1$}, $m$ is the mass of the particle 
and $c$ is the speed of light in vacuum. By contracting\footnote{Using the particle physicists convention of the Minkowski metric tensor $\eta_{\mu\nu}$,  (+, -, -, -)} two four-momenta we get the important Lorentz invariant 
square of the \textit{invariant mass}
$$
    m^2 = p_\mu p^\mu = E^2 - \abs{\mathbf{p}}^2 
$$
which can be generalized for a system containing $n$ particles as
\begin{equation}\label{eq:invariant_mass}
    m^2 = p_\mu p^\mu = \left(\sum_{i=1}^n E_i\right)^2 - \left(\sum_{i=1}^n\mathbf{p}_i\right)^2
\end{equation}
As this thesis will focus on a dilepton (and missing transverse energy) final state, which is of the type $2\rightarrow4$ where two are invisible, but the invariant mass of the two visible leptons in the final state will be of interest, 
we will denote the invariant mass of the visible leptons as $m_{ll}$. From this we can also get another interesting variable, the \textit{transverse energy}. This follows directly from Eq. (\ref{eq:invariant_mass}) by using the transverse momentum
\begin{equation}\label{eq:transverse_energy}
    E_T = \sqrt{m^2 + p_T^2}
\end{equation}
The invariant mass is what we measure in the final state only. 
As we analyze data\footnote{And mostly simulations mimicking the ATLAS detector} from Run II on LHC, which operated at $\sqrt s = 13$ TeV, it is important to consider the determination of the center-of-mass energy, which characterizes the initial state of the colliding protons. 
While the LHC controls the initial state, it is crucial to understand that the 13 TeV energy value associated with the incoming protons does not directly define the center-of-mass energy of the initial state. Instead, it is the partons, 
primarily gluons but also including quarks and anti-quarks, that contribute to the center-of-mass energy, see Section \ref{sec:pdf} for more details.\\
% These partons have the ability to possess any collision energy below 13 TeV. The momentum fractions of the partons are determined by the product of 
% the invariant mass and the pseudorapidity $(m/\eta)$ of the final state particles, for more info on partons see Chapter \ref{sec:pdf}.\\
\\The aim of this thesis is to search for DM, which we know interacts weakly with matter in the "same way" as neutrinos, meaning it leaves no signal in detectors such as ATLAS\footnote{Neutrinos interact in dense and large mediums however}. So how can we detect its presence? 
As we know that the transverse momenta before a particle collision is zero\footnote{Because protons travel along the beam axis}, then we know that the sum of transverse momenta must be zero after the collision as well, to conserve momentum.
From this we often can infer the presence of the non-interacting particles from the presence of \textit{missing transverse energy}\footnote{Also called \textit{missing momentum}. While energy is a scalar quantity without direction, we use the term "missing transverse energy" to emphasize the magnitude of the missing transverse momentum vector. This convention is adopted due to the nature of our energy measurements in calorimeters, which actually give us directions} 
(MET), which is defined by 
\begin{equation}\label{eq:MET}
    E_T^{miss} = \abs{\mathbf{p}_T^{miss}},\quad\text{where }\quad \mathbf{p}_T^{miss} \equiv -\sum_i \mathbf{p}_{T,i}
\end{equation}
where the sum extends over the measured momenta of all the observed particles in an event, as well as all the low energy tracks which are reconstructed, but not associated with any particle\footnote{Referred to as \textit{soft terms}}. From this formula, if all particles produced in the collision have been detected, then this sum should be zero. Meaning that 
in ideal cases, significant MET is an indicative of the presence of undetected particles. \\
\\Another useful kinematic variable is the \textit{hadronic activity} which is the scalar sum of the transverse momentum of all jets in an event, defined as
\begin{equation}\label{eq:HT}
    H_T = \sum_{i\in\{jets\}} \vert\vert \mathbf{p}_{T,i}\vert\vert
\end{equation}
This gives a measurement of the hadronic energy scale of an event. Another handy trick comes from the realization that the centre-of-mass frame is between the hadrons in jets, where the total momentum is given as a function of the energy of the hadrons. 
This means that the final state particles are boosted along the beam axis. With this realization we can now introduce a Lorentz invariant\footnote{Under boosts along the beam axis} kinematic property known as the \textit{rapidity, y} used to express the polar angles
\begin{equation}\label{eq:rapidity}
    y \equiv \frac{1}{2}\ln\left(\frac{E+p_z}{E-p_z}\right)  
\end{equation} 
We can use that $p_Z = E\cos\theta$ in the high-energy limit as the mass is negligible. In this limit we can use the \textit{pseudorapidity}, $\eta$, defined by
\begin{equation}\label{eq:pseudorapidity}
    y\approx \eta \equiv -\ln\left(\tan\frac{\theta}{2}\right)
\end{equation}
The pseudorapidity is an interesting variable as, in the same was as $\theta$, it can tell us how close to the beam the final state particles are\footnote{Where the higher $\vert\eta\vert$ means closer to the beam}, but this time independently of the boost as it does not include the mass of the particle. The
pseudorapidity also gives us that for a single particle the phase space is more uniformly distributed on $(\eta,\phi)$ than $(\theta,\phi)-$space. The $\eta$ variable can also be negative, meaning it is boosted along the other beam direction. 
From this we can define a new variable which will come handy with particle identification, which is called the \textit{R-cone}, that defines a circle in $(\eta,\phi)$-space. It is defined as
\begin{equation}\label{eq:R-cone}
    \Delta R = \sqrt{(\Delta\eta)^2+(\Delta\phi)^2}
\end{equation}
Another interesting variable is the \textit{transverse mass}, defined as
\begin{equation}\label{eq:transverse_mass}
    m_T^2 = m^2 + p_T^2
\end{equation}
where $m^2$ is the invariant mass defined in Eq. (\ref{eq:invariant_mass}). What is interesting about this variable is that it is a generalization of the invariant mass in the transverse plane and can thus accommodate the MET coming from invisible particles.
We can take this further by looking at a variable which calculates a transverse mass for two leptons by distributing the total $p_T^{miss}$ among the two systems, 
and minimizing the maximum of the two transverse masses by varying the distribution of the $p_T^{miss}-$vector in terms of the size of $p_T$. This is called the \textit{stransverse mass} and is defined by
\begin{equation}\label{eq:stransverse_mass}
    m_{T2}^2(\chi) = \underset{\slashed{\mathbf{q}}^{(1)}_T + \slashed{\mathbf{q}}^{(2)}_T = \slashed{\mathbf{p}}_T}{\min}
    \left[\max \left\{m_T^2\left(\mathbf{p}_T^{\ell_1}, \slashed{\mathbf{q}}^{(1)}_T;\chi\right), m_T^2\left(\mathbf{p}_T^{\ell_2}, \slashed{\mathbf{q}}^{(2)}_T;\chi\right) 
    \right\}\right] 
\end{equation}
where $\slashed{\mathbf{q}}_T$ are "dummy 2-vectors", $\chi$ is a free parameter used to "guess" the mass of the invisible particle, and $m_T^2\left(\mathbf{p}_T, \mathbf{q}_T\right)$ is an application of 
Eq. (\ref{eq:transverse_mass}) using two particles:
$$
m_T^2 \left(\mathbf{p}_T, \mathbf{q}_T\right) = 2(p_T q_T - \mathbf{p}_T\cdot\mathbf{q}_T)
$$
For a more detailed explanation and interpretation of the stransverse mass we refer the reader to the paper by Barr et al. \cite{Barr_2003}. Even though the stransverse mass was made with neutralinos in mind, it can still 
be used to calculate SM processes. For example, if we want to reduce $W^+W^-$ background events, we can first recall that each boson can decay as $W^+\rightarrow l^++\nu_l$ (and $W^-\rightarrow l^-+\overline{\nu}_l$) with the $W$ mass, $m_W$, as an endpoint. 
Meaning that we can use $m_{T2}$ to reduce the $W^+W^-$ events in a dilepton final state by requiring that $m_{T2} > m_W$.

\subsection{Proton-proton collisions}
With all the kinematics out of the way the question of how the particles are produced still remains. The answer could be an electron-positron collider, as they did in LEP, a proton anti-proton collider, like the Sp$\overline{\mbox{p}}$S. As this thesis uses LHC data we will look at proton-proton collisions.
Protons are made of elementary particles, two \textit{up-} and one \textit{down-}quarks to be specific. Because of this it is not hard to realize that the Feynman rules acquired from the SM (Figure \ref{fig:feynman_rules}) also apply here. In this subsection we will study how different effects of $pp-$collisions affect 
the cross-section, and therefore the expected number of events to occur, from a kinematical point of view.

\subsubsection{Parton Distribution Functions}\label{sec:pdf}
Although the proton is made up of two up quarks and one down quark, called the \textit{valence quarks}, the proton also consists of gluons and other quarks and anti-quarks, called \textit{sea quarks}. These sea quarks become important in deep inelastic scattering, 
where the proton breaks apart due the high energies in the collisions. As we accelerate the protons before colliding them, we also accelerate the quarks and "gluons" inside it, each of the partons carry a momentum fraction $x$ of the proton, 
referred to as the Bjorken $x$. We can then calculate the reduced centre-of-mass, $\hat{s}$ of two colliding partons $q_1$ and $q_2$, as seen in Figure \ref{fig:Feynann_PDF}, with the proton momentum fraction $x_1$ and $x_2$, from the proton's momentum $p_1$ and $p_2$ respectively as
$$
\hat{s}=x_1x_2s
$$
where $s$ is the centre-of-mass energy squared of the $pp-$system. The valence quarks in the proton do not only interact with the other valence quarks in the other proton, but they might also emit gluons which split into quark anti-quarks pairs 
or gluon-gluon, making a “sea” of gluons, quarks and anti-quarks around the valence quarks. The momentum of the partons inside the proton are dependent on the momentum transfer $Q^2$ and is represented by an experimentally determined momentum distribution, 
known as the \textit{parton distribution function} (PDF) \cite{PDF} $f(x,Q^2)$. In other words, the PDFs give the probability of a parton to carry the momentum fraction $x$ of a proton. The shape and form the PDFs play 
an important role in estimating the process cross-section that occur after the $pp-$collisions, and therefore are crucial when simulating events using Monte Carlo \cite{MC_PDF}.  If we take as an example the process 
$pp\rightarrow l^+l^-+X$ where $X$ denotes any hadrons formed by the remaining quarks. Figure \ref{fig:Feynann_PDF} showcases the process.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.75\textwidth]{PP_Feynmann_Diagram.png}
    \caption[Feynman diagram from $pp-$collision]{Feynman diagram depicting the $pp\rightarrow l^+l^-+X$ process where $X$ denotes any hadrons formed by partons
    not taking part in the hard process $q\bar{q}\rightarrow l^+l^-$. The colliding valence quarks are defined as $q_i=x_ip_i$ where each quark $q_i$ is carrying the momentum fraction $x_i$ of the proton momentum $p_i$.}\label{fig:Feynann_PDF}
\end{figure}
\\The cross-section for the process is 
$$
\sigma\left(p(p_1) p(p_2)\rightarrow l^+l^- +X\right)=
$$
\begin{equation}\label{eq:PDF_xsec}
    \int_{0}^{1}\int_{0}^{1}\sum_{f}f_f(x_1)f_{\bar{f}}(x_2)\cdot\sigma\left(q_f(x_1p_1)+\bar{q}_f(x_2p_2)\rightarrow l^+l^- \right) dx_2dx_1
\end{equation}
where $\sigma\left(q_f(x_1p_1)+\bar{q}_f(x_2p_2)\rightarrow l^+l^- \right)$ is calculated using the Feynman rules given in Figure \ref{fig:feynman_rules}, and $f$ are the PDFs. %As we can see this can drastically change the cross-section, and with that the 
% probability, for an event we want to study. 

\subsubsection{Breit-Wigner resonance}
Another important aspect when looking at particle collisions is the \textit{Breit-Wigner resonance}. All unstable particles have a decay rate or width, $\Gamma$, (given as inverse of the lifetime $\tau$) which is present in the wave function, 
$\Psi \propto e^{-i(m-i\Gamma/2)}$. This decay rate also becomes apparent when the unstable particle is the propagator of the interaction we are studying 
\begin{equation}\label{eq:Breit-Wigner}
    \sigma \propto\frac{1}{(s-m^2)^2-m^2\Gamma^2}
\end{equation}
From this we can see that, as the square of the centre-of-mass energy $s$, approaches the unstable particles mass $m$, there will be a resonance at the invariant mass of the final state particles that "show" the mass of the unstable particle, this is called the Breit-Wigner resonance. 
It is because of resonances like this that we can identify particles such as the $Z$ boson. A new resonance $Z'$ resonance is predicted by some BSM DM models studied in this thesis.\\


\subsubsection{Expected events}
The most important value we need to know when studying $pp-$collisions is the number of events $N$ expected for a process, this is defined as
\begin{equation}\label{eq:expected_events}
    N=\sigma\int\mathcal{L}(t)dt, \qquad\text{where}\quad \mathcal{L}=f\frac{n_1n_2}{\sigma_x\sigma_y}
\end{equation}
where $\sigma$ is the cross-section of the process as expressed in Eq (\ref{eq:cross_sec}) while also taking into account the PDF functions in Eq. (\ref{eq:PDF_xsec}) and the decay widht in Eq. (\ref{eq:Breit-Wigner}).
The last three symbols come from accelerator kinematics where $\sigma_{x,y}$ denotes the beam size, $f$ is the frequency of bunch crossings, and $n_{1,2}$ is the number of protons in bunches.





\clearpage
\section{The ATLAS detector}\label{sec:ATLAS}
We have so far in this chapter discussed how particles are produced. But we have not yet explained how we actually detect them, arguably the most important matter in the field of experimental high energy particle physics.
This section of the chapter is just about that, and we will explain how the detection happens in A Toroidal LHC ApparatuS, or more commonly know as the ATLAS detector. Figure \ref{fig:ATLAS_detector} showcases 
the detector and its size. The information of this section is largely based on the original ATLAS Technical Design Report \cite{Aad:1129811}.\\
\\The ATLAS detector is a general multipurpose\footnote{Probing $pp-$ and $AA-$ (heavy ions) collisions.} detector located at the LHC and covers nearly the entire solid angle 
around the collision point, as described in the reference frame depicted in Figure \ref{fig:ATLAS_detector}. The ATLAS detector consists of four main subdetectors; ($i$) an inner tracking detector (ID), 
an ($ii$) electromagnetic calorimeter (ECAL), a ($iii$) hadronic calorimeter (HCAL), and lastly ($iv$) a muon spectrometer (MS). Figure \ref{fig:ATLAS_layers} visualizes in the transverse plane the four $(i)-(iv)$ main sub-detectors, 
along with how the different particle types interact with each layer. A brief description of each layer is given in this section.
\begin{figure}[!ht]
	\centering
    \includegraphics[width=1\textwidth]{ATLAS_detector.jpg}
    \caption[The ATLAS detector]{Cut-away view of the ATLAS detector, image taken from Ref. \cite{Aad:1129811}}\label{fig:ATLAS_detector}
\end{figure}
\begin{figure}[!ht]
	\centering
    \includegraphics[width=0.80\textwidth]{ATLAS_detection.jpg}
    \caption[Illustration of the ATLAS detector layers]{Illustration of the ATLAS detector layers, image taken from Ref. \cite{Pequenao:1505342}}\label{fig:ATLAS_layers}
\end{figure}
\\
\subsection{Inner detector}\label{sec:ID} 
The inner-detector (ID) system is immersed in a 2T axial magnetic field and provides charged-particle tracking in the range $\abs{\eta}<2.5$. The ID provides the first measurements of the 
momentum and charge of electrically charged particles, as these can be determined by the curvature of their reconstructed tracks. 
The ID is made of three independent systems; the pixel detector, the semiconductor tracker (SCT) and 
the transition radiation tracker (TRT). \\
\\The pixel detector is made up of 80 million silicon pixel sensors, each of size $50\times400\mu$m$^2$, %with a resolution of $14\times155\mu$m$^2$, 
and spread over four layers. Outside the pixel layers 
are the SCT, which consists of silicon microstrip detectors, also placed on multiple layers. The SCT covers the pseudorapidity region $\abs{\eta}<2.5$. Lastly we have the TRT, 
which consist of 4 mm in diameter straw tubes, which enable track-following up to $\abs{\eta}=2.0$ and allows for electron identification. 

\subsection{The calorimeters}\label{sec:calories}
The ATLAS detector has two types of calorimeters, the \textit{electromagnetic calorimeter} (ECAL) and the \textit{hadronic calorimeter} (HCAL), both designed to fully stop certain types of particles. 
The ECAL is immediately surrounding the inner detector and is divided into a barrel part ($\abs{\eta}<1.475$) and two end-cap components ($1.375<\abs{\eta}<3.2$). 
The ECAL consists of absorbing lead plates, with liquid Argon (LAr) in between. The thickness of the calorimeter is made to fully measure the shower of photons and electrons/positrons. The muons will only lose 
a small fraction of their energy as they have longer interaction lengths in lead.\\
\\The HCAL is immediately surrounding the ECAL on all sides and consists of two types of detectors. In the barrel ($\abs{\eta}<1.0$) and the extended barrel regions ($0.8<\abs{\eta}<1.7$), the HCAL is made of 
steel plates with plastic scintillator tiles as active material. While on the end-cap regions ($1.5<\abs{\eta}<3.2$) there are hadronic LAr detectors, with absorbing copper plates as active material; in the 
forward region ($3.1<\abs{\eta}<4.9$) a combination of copper and tungsten plates are used as active material. The active materials are chosen to maximize the interaction cross-section with hadrons, such as neutrons, 
protons and charged pions. The depth of the HCALs is also designed to fully stop hadrons and their showers, which corresponds to the deposited energies. Hadrons are efficiently stopped at the HCALs, meaning that only 
muons and invisible particles, such as neutrinos and potentially dark matter, leave the HCAL.

\subsection{Muon spectrometer}\label{sec:MS}
The outermost layer of the ATLAS detector is the \textit{muon spectrometer} (MS), dedicated to the measurement and identification of the muons momenta. The MS consists of multiple layers of detector material, 
and is immersed in a strong magnetic field to bend the trajectories of the charged muons. The MS is made of four different types of detector component: $(i)$ Monitored Drift Tubes (MDTs) in the barrel, 
$(ii)$ Cathode Strip Chambers (CSCs) dealing with the events closer to the beam line in the end cap, $(iii)$ Resistive Plate Chambers (RPCs) in the barrel and ($iv$) Thin Gap Chambers (TGCs) in the end caps. 
The MDTs and CSCs are used for tracking while the RPCs and TGCs are used for triggering. The tracking is provided for pseudorapidities up to $\abs{\eta}<2.7$, and the trigger system only extends to $\abs{\eta}<2.5$.

\section{Data analysis}\label{sec:data_anal}
The time has come to explore how we can search for new physics phenomena, now that it has been established how particles are produced and how we detect them. In this section we will take into account the classic way of searching for new physics which is called 
the \textit{cut and count method}, but there are other methods to search for new physics, such as Machine Learning (ML) which is the method pursued in this thesis. There might be other methods, such as 
Quantum ML, but as of today we are still in a too early stage of the technology \cite{QML}. To give a short description of the cut and count method, it makes kinematical \textit{cuts} on various variables to isolate signal from background. The way 
the signal and background\footnote{There are also data driven methods, but on this thesis we will focus on simulations} are made is by Monte Carlo (MC) simulations, which is necessary for guiding us on where to place efficient cuts and understand our sensitivity to a given BSM model. As it would not be a real experimental physics discovery without making a statistical 
analysis of the results we will also explain how we utilize this tool. To guide us through this process we will use $ZZ^{(*)}$ channel in the discovery of the Higgs Boson in 2012 \cite{Higgs_discovery_2012} as an example of the success of the cut and count method, and 
the search for the existence of a new $W'$ boson studied by ATLAS \cite{Stat}, for the statistical tools.

\subsection{Cut and count method}
The cut and count method is what has been the standard method of doing data analysis with LHC data. As the name implies, the cut and count method works by making cuts on kinematical variables and afterwards counting how many events are left. 
The goal of using this method is to make cuts such that we remove as many background events as possible while also keeping as many signal events as possible. For example, if we were to study a new physics model with a new light vector boson behaving similarly to the $Z$ boson, but with a higher mass, 
then a good kinematical cut to remove many background processes would be to require that $m_{ll}>100$ GeV, as this would remove the majority of $Z$-resonance from the final state, making it easier to "find" the new physics model. \\
\\To more thoroughly explore the cut and count method we can look at the Higgs discovery, in particular the $H\rightarrow ZZ^{(*)} \rightarrow 4l$\footnote{Where $l$ is for lepton, but only means $e^\pm$ or $\mu^\pm$} channel. 
The event selection consist of kinematical cuts used. The kinematical cuts included (aside from the \textit{acceptane cuts}), were:
\begin{itemize}
    \item Single-lepton or dilepton triggers
    \item Four leptons final state with ordered $p_T >20,15,10,7$ GeV
    \item Higgs-boson candidates are formed by selecting two same flavor opposite charge lepton pairs
\end{itemize}
The first "cut" is to make sure that the event contains leptons with sufficient quality to have fired one of the relevant triggers. The second cut is to have the sufficient number of leptons in the final state with $p_T$ above the trigger thresholds. And lastly we want to have two lepton pairs of the same-flavor 
with opposite-charge, this is to make sure that one lepton pair actually decays from a $Z$-boson. What now remains is to explore the "count" part of this method. When counting the events that pass the event selection one usually counts the background events that pass, the data 
points that pass, and also the signal events that pass. For the 2012 Higgs discovery, the Higgs channel with a Higgs mass of 125 GeV was used as signal. 
The results of the 2012 discovery is shown in Figure \ref{fig:Higgs_ZZ}.\\
\begin{figure}[!ht]
	\centering
    \includegraphics[width=0.5\textwidth]{Higgs_ZZ_discovery.png}
    \caption[The Higgs discovery on the $ZZ^{(*)}$ channel]{The Higgs discovery on the $ZZ^{(*)}$ channel, image taken from Ref. \cite{Higgs_discovery_2012}}\label{fig:Higgs_ZZ}
\end{figure}
\\Although this section made the process look simple, it was through the effort of many scientists working together that made this happen. The great computational power needed to \textit{correctly} simulate events and reconstruct objects from detector signal was, and still is a big challenge.
Not to mention the state-of-the-art technology to be able to both accelerate the protons to an energy high enough to "create" new physics, and to actually be able to detect it. This alone was not enough to claim the discovery, 
to claim anything we need to look at statistics, which is the subject of the next section.


\subsection{Statistical tools}\label{sec:stat_anal}
To make any sort of claim in modern physics we should be absolutely certain that what we are claiming is true, as just making the cuts and isolating a signal to background is not enough. To be specific we need to be "\textit{at least $5\sigma$ sure}" to claim any new discoveries. 
But as not everything in experimental high energy particle physics is a new discovery, we will also look at \textit{exclusions}. In this section we will present some mathematical details behind the statistical tools that will be used in this thesis.\\ 
\\This thesis follows the example set by Dr. Bugge in the FYS5555 course at UiO \cite{magnar_pensum}, here we use as an example the 8 TeV ATLAS $W'$ search \cite{Stat},
where a Bayesian analysis was performed to set limits on the existence of a new $W'$ boson. Using the signal+background hypothesis, the expected number of events in each lepton channel, $ee, \mu\mu$, of the process $W'\rightarrow l\nu$ is 
$$
    N_{\text{exp}} = \varepsilon_{\text{sig}}L_{\text{int}}\sigma B + N_{\text{bkg}}
$$
where $L_{\text{int}}$ is the integrated luminosity, $\varepsilon_{\text{sig}}$ is the signal selection efficiency defined as the fraction of signal events that satisfy the event selection criteria, $N_{\text{bkg}}$ is the expected number of background events, and $\sigma B$ is the 
cross-section times branching ratio of the process. Using Poisson statistics, the likelihood to observe $N_{\text{obs}}$ events is
\begin{equation}\label{eq:observed_events_prob}
    \mathcal{L}(N_{\text{obs}}\vert \sigma B) = \frac{(N_{\text{exp}})^{N_{\text{obs}}}e^{-N_{\text{exp}}}}{N_{\text{obs}}!}
\end{equation}
We include uncertainties by introducing nuisance parameters $\theta_i$, each with a probability density function $g_i(\theta_i)$, and integrating the product of the Poisson likelihood with the probability density function. The integrated likelihood is
\begin{equation}\label{eq:observed_events_nuisance}
    \mathcal{L}_B(N_{\text{obs}}\vert \sigma B)=\int\mathcal{L}(N_{\text{obs}}\vert \sigma B)\prod g_i(\theta_i)d\theta_i
\end{equation}
where a log-normal distribution is used for the $g_i(\theta_i)$. The nuisance parameters are taken to be: $L_{\text{int}}, \varepsilon_{\text{sig}}$ and $N_{\text{bkg}}$. The measurements of 
the two decay channels (muon or electron final state for $W'\rightarrow l\nu$) are combined assuming the same branching fraction for each, thus Eq. (\ref{eq:observed_events_nuisance}) remains valid with the Poisson likelihood replaced by the product of the Poisson likelihoods for the two channels. 
The integrated luminosities for the electron and muon channels are fully correlated. We can further use Bayes' theorem which gives the posterior probability that the signal has signal strength $\sigma B$:
\begin{equation}\label{eq:prior_prob}
    P_{\text{post}}(\sigma B\vert N_{\text{obs}}) = N \mathcal{L}_B (N_{\text{obs}}\vert\sigma B) P_{\text{prior}}(\sigma B)
\end{equation}
where $P_{\text{prior}}(\sigma B)$ is the assumed prior probability, here chosen to be flat in $\sigma B$, for $\sigma B$ > 0. The constant factor $N$ normalizes the total probability to one. The posterior probability is evaluated for each mass and decay channel as well as for their 
combination, and then used to set a limit on $\sigma B$.\\
\\As we can see, the inputs for the evaluation of $\mathcal{L}_B$ (and $P_{\text{post}}$) are $\varepsilon_{\text{sig}}$, $L_{\text{int}}$, $N_{\text{bkg}}$ and $N_{\text{obs}}$ and the uncertainties of the first three. The uncertainties for these should account for experimental 
and theoretical systematic effects as well as the statistics of the simulated samples. For this thesis the systematic uncertainties will not be calculated, but will rather be assumed to be flat and $\pm$ 20\% of the background.\\
\\To make exclusions we can use Eq. (\ref{eq:prior_prob}) to establish a \textit{confidence level} (CL). CLs are defined as the probability to observe the number of events observed in an experiment, $N_{\text{obs}}$, or \textit{less} given signal+background. We usually define a signal+background 
hypothesis to be excluded when CL$_{s+b}$ < 5\%. Meaning a 95\% CL, such that the probability to falsely exclude an existing signal(+background) is 5\%. We will use CL is to \textit{set limits} on theoretical models, rather than exclude them.\\
\\On the other side, to claim any discovery in particle physics we need to know the \textit{significance} of any statistical fluctuation. Before getting to the significance we can discuss the \textit{p-value}, defined as the probability to observe the number of events observed in the experiment, $n_{\text{obs}}$, 
or \textit{more} given only background
\begin{equation}\label{eq:p-value}
    p = P(N\ge N_{\text{obs}}\vert \lambda = N_{\text{bkg}}) = \sum_{k=N_{\text{obs}}}^{\infty}\mathcal{L}(k\vert N_{\text{bkg}})
\end{equation}
The smaller the $p-$value, the less compatible an observation is with the background only hypothesis, meaning more likely to be a discovery. In Figure \ref{fig:p-val} we can see how the $p-$value could look for an arbitrary distribution
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{p-val_Z.png}
    \caption[$p-$value and significance $Z$ relation]{Illustration showing how the $p-$value of an arbitrary distribution might look (shaded area). We can also see the relation of the $p-$value to the significance $Z$ in this illusteation.}\label{fig:p-val}
\end{figure}
\\From this figure we also see the relation between the $p-$value and the significance $Z$. Mathematically the relation is expressed by
$$
    p = \int_{-\infty}^{-Z} \frac{e^{-x^2/2}}{\sqrt{2\pi}}dx
$$
As mentioned in the start of this subsection, a discovery in particle physics is defined to be at least a $Z=5\sigma$ deviation from the background hypothesis, meaning that we would have a $p-$value of $p\le2.87\times10^{-7}$. In other words, with a $5\sigma$ deviation, the probability to 
falsely discover something is at worst one in roughly 3.5 million.\\ 
\\As the significance is an interesting quantity we can give it its own definition. We will use the low statistics formula for the significance, 
as this is the most general one. We can either define the significance as the \textit{observed significance} with the equation
$$
    Z = \sqrt{2\left[N_{\text{obs}}\ln\frac{N_{\text{obs}}}{N_{\text{bkg}}}-N_{\text{obs}}+N_{\text{bkg}}\right]}
$$
However, for our purposes, as we will use the signal+background hypothesis, we will use the \textit{expected significance}, which we get by changing $N_{\text{obs}} \rightarrow N_{\text{sig}}+N_{\text{bkg}}$, where $N_{\text{sig}}$ is the number of signal events
\begin{equation}\label{eq:exp_sig}
    Z = \sqrt{2\left[(N_{\text{sig}}+N_{\text{bkg}})\ln\left(1+\frac{N_{\text{sig}}}{N_{\text{bkg}}}\right)-N_{\text{sig}}\right]}
\end{equation}
However, as Eq. (\ref{eq:p-value}) did not include any nuisance parameters, it used Eq. (\ref{eq:observed_events_prob}) instead of Eq. (\ref{eq:observed_events_nuisance}). We want to express the significance with uncertainties. From "Discovery sensitivity for a counting experiment with background uncertainty" from Glen Cowan
\cite{Cowan_Uncertainty_in_sig}, we can use then Eq. (17) on his paper that reads
$$
Z = \left[-2\left(N_{\text{obs}}\ln\left[\frac{N_{\text{obs}}+m}{(1+\tau)N_{\text{obs}}}\right] + m\ln\left[\frac{\tau(N_{\text{obs}}+m)}{(1+\tau)m}\right]\right)\right]^{1/2}
$$
where $m=\tau N_{\text{bkg}}$ and where we have Eq. (19) on his paper that says
$$
\tau=\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2}
$$
where $\sigma_{\text{bkg}}$ is the uncertainty of the background. Using the prior definitions of $m$ and $\tau$, as well as changing $N_{\text{obs}} \rightarrow N_{\text{sig}}+N_{\text{bkg}}$ gives us
\begin{equation}\label{eq:significance}
    \textstyle    Z = \sqrt{-2\left((N_{\text{sig}}+N_{\text{bkg}})\ln\left[\frac{(N_{\text{sig}}+N_{\text{bkg}})+\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} }{(1+\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2})(N_{\text{sig}}+N_{\text{bkg}})}\right] + \frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} \ln\left[\frac{\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2}((N_{\text{sig}}+N_{\text{bkg}})+\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2} )}{(1+\frac{N_{\text{bkg}}}{\sigma_{\text{bkg}}^2})\frac{N_{\text{bkg}}^2}{\sigma_{\text{bkg}}^2}}\right]\right)}
\end{equation}
Which makes for a better estimate of the significance one has in reality.
\newpage\noindent The 95 \% CL limit results from the 8 TeV ATLAS $W'$ search \cite{Stat} can be seen in Figure \ref{fig:HIGGS_CL}. To understand the exclusion plot here we will explain what the objects are. The y-axis of the plot represents the cross-section times branching ratio of the process. 
The x-axis is the mass of the $W'$ boson. The red dashed line is the predicted theoretical cross-section times branching ratio. The black dots represent the observed exclusion limit gathered from real data, in this thesis we will not include real data in the exclusion plots we generate. 
The dashed black line is the expected 95\% CL limit calculated using the tools presented in this section, with a 1$\sigma$ (green) and 2$\sigma$ (yellow) variance. The way we set limits on the models is by excluding all the mass points that have a higher predicted cross-section times branching ratio than the expected limit. 
This means that the 8 TeV ATLAS $W'$ search excluded the mass of the $W'$ up to $\approx3100$ GeV, as this is where the red dashed line crosses the black dashed line in Figure \ref{fig:HIGGS_CL}.
\begin{figure}[!ht]
	\centering
    \includegraphics[width=0.7\textwidth]{CL_Wp.png}
    \caption[95\% CL example using the ATLAS $W'$ search at 8 TeV]{95\% CL example using the ATLAS $W'$ search at 8 TeV, figure taken from Ref. \cite{Stat}}\label{fig:HIGGS_CL}
\end{figure}
% \\The exclusion plot in Figure \ref{fig:HIGGS_CL} is great at excluding models with one varying parameter. However, as we will look at the direct slepton production model and the 2HDM + a model, which have two varying parameters, we will also make 2D-exclusion plots. For this thesis we used the 
% tools made by Dr. Gramstad, available here \cite{eirik_pensum}.

\clearpage
\section{Summary}
In this chapter, we have studied how one can discover new physics from calculating the number of expected events from $pp-$collisions. Using special relativity as a tool, we can express the four-momentum of the particles with 
detector-coordinates, $p^\mu=(E,p_T\cos\phi,p_T\sin\phi,\abs{p}\cos\theta)$. From this four-momentum vector we can thereafter calculate interesting kinematic variables such as the invariant mass $m_{ll}$, missing transverse energy $E_T^{miss}$, 
stransverse mass, $m_{T2}$, among others. Using the advanced ATLAS detector with the main four parts; \textit{inner-detector} (ID), \textit{electromagnetic calorimeter} (ECAL), \textit{hadronic calorimeter} (HCAL), and the \textit{muon spectrometer} (MS), 
we can get experimental measurements from the particles that come from the accelerated protons at the LHC.\\
\\With the recorded data and MC simulations taking into account the experimental features such as the PDFs and Breit-Wigner resonance, as well as the ATLAS kinematics, we can compare how the simulated events fare with the data recorded. 
By playing around with the kinematical variables of the particles and making cuts to isolate new physics signal, we can see if there is a discrepancy between the data recorded and the SM background. After creating this signal region with the cut and count 
method we conduct a statistical analysis to see how the new theory/observed data deviates from our current understanding of physics.\\
\\This state-of-the-art method is what currently is being used at CERN and has lead to a great advancement in the field. However, with the rise of new technologies, such as \textit{machine learning}, which excel at classification tasks, a door has been 
opened to try new methods. In this thesis we will use ML to hopefully create a better and more general signal region than what the current cut and count method does. Before describing how, we will explain what machine learning is, this is the subject 
of the next chapter.
\end{document}