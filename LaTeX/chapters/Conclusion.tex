\documentclass[12pt, a4paper]{book}
\begin{document}\textbf{NB: This will be changed with the new results!}
After exploring the intricacies of preparing both a NN and a BDT ML algorithm to learn a binary classification task of DM signal and SM background, we concluded, due to time, that a BDT ML approach was 
best suited for this search. The searches we conducted were made on three models all containing a new $U(1)'$ vector boson, $Z'$, that can decay and couple to a DM WIMP candidate. The first one being $Z'$ coupling to a new scalar boson $h_D$, 
which we call the Dark Higgs (DH) model. The second is an off-shell $Z'$ decaying into two dark states $\chi_1$ and $\chi_2$, where the latter decays again into a $Z'$ and $\chi_1$ which is the DM candidate, this we called the Light Vector (LV) model. 
The third model, is an inelastic Effective Field Theory model, which is similar to the LV model, with the exception being that there are no assumptions about the quark coupling to the $Z'$, we called this the EFT model. The three models were furthermore 
split into two groups, the difference being the mass of the DM candidate, we called these for the Heavy- and Light Dark Sector (HDS and LDS) for heavy and light DM respectively.\\
\\As the ultimate goal for the thesis was to test model independent approaches for new physics searches using ML, we first tested a less model dependent approach than is the standard in high energy physics today. This approach consisted of using a dataset 
with all the SM background processes with a dilepton and MET final state and one of the aforementioned models, including all the simulated $Z'$ mediator masses\footnote{As the standard approach is to train using only one simulated mediator mass}. 
To better the performance and computational time, we only looked at events with MET > 50 GeV, and as the models tested had a lower limit of the mediator mass at $m_{Z'}=130$ GeV, we also only looked at events with $m_{ll}>110$ GeV. Then we trained one BDT 
for each model using this type of dataset. To test the results of this approach we computed mass exclusion limits using Bayesian statistics for every model.\\
\\The second approach consisted of a dataset containing all the DM models and all the SM background. The difference however was that we trained three BDTs in orthogonal MET spaces. All the networks only looked at events with, again $m_{ll}>110$ 
GeV, while the other three were divided into three MET regions, the first signal region (SR1) having $E_T^{miss}\in[50,100]$ GeV, the second signal region (SR2) having $E_T^{miss}\in[100,150]$ GeV, and the third signal region (SR3) having $E_T^{miss}>150$ 
GeV. After training the networks we tested each model by computing the same mass exclusion limit using Bayesian statistics in every respective SR. To compare this approach with the previous, we statistically combined the mass exclusion limit of all SRs into 
one combined SR. \\
\\Doing this for every model we observed that we were able to compute higher mass limits using the more model independent approach of dividing a dataset into different regions of kinematical phase space. While we had originally planned to test these methods 
using more DM models, such as the Mono-Z and SUSY, due to time we were not able to test if the model independent approach worked using more experimentally distinct models. For the future it would be interesting to see if this approach could be used to train 
less ML networks and still get good, if not better results as demonstrated by the few models in this paper, when conducting search for new physics which might get us closer to understanding more of spacetime.
\end{document}