\documentclass[12pt, a4paper]{book}
\begin{document}

\chapter{Algorithms for BDTs and NNs}\label{apendix:Algorithms}

\begin{lstlisting}[basicstyle=\tiny, language=Python, caption={Neural network definition using TensorFlow}, label=alg:nn, captionpos=t]
    import tensorflow as tf
    from tensorflow.keras import layers
    
    def Neural_Network(inputsize, n_layers, n_neuron, eta, lamda):
        
        model=tf.keras.Sequential()      
        
        for i in range(n_layers):       
            if (i==0):                  
                model.add(layers.Dense(n_neuron, activation='relu', kernel_regularizer=
                 tf.keras.regularizers.l2(lamda), input_dim=inputsize))
            else:                       
                model.add(layers.Dense(n_neuron, activation='relu', kernel_regularizer=
                        tf.keras.regularizers.l2(lamda)))
                        
        model.add(layers.Dense(1,activation='sigmoid')) 
        
        opt=tf.optimizers.SGD(learning_rate=eta) # or Adam!
        
        model.compile(loss=tf.losses.BinaryCrossentropy(),
                    optimizer=opt,
                    metrics = [tf.keras.metrics.BinaryAccuracy()])
        return model
\end{lstlisting}

\begin{lstlisting}[basicstyle=\tiny, language=Python, caption={Boosted Decision Tree definition using XGBoost}, label=alg:xgb, captionpos=t]
    import xgboost as xgb
    
    Boosted_Decision_Tree = xgb.XGBClassifier(
                 max_depth, 
                 use_label_encoder=False,
                 n_estimators,
                 learning_rate,
                 reg_lambda,
                 predictor = 'cpu_predictor',
                 tree_method = 'hist',
                 scale_pos_weight = sow_bkg/sow_sig,
                 objective = 'binary:logistic',
                 eval_metric = 'auc',
                 min_child_weight = 1,
                 missing = -999,
                 random_state = 42,
                 verbosity = 1) 
 \end{lstlisting}


\end{document}