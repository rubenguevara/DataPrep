\documentclass[12pt, a4paper]{book}
\begin{document}\label{chap:Method_ML}
We have now presented the dataset and theory, and are thus ready to start making our ML algorithms. The first part of this chapter will be a presentation of the dataset in more details, included all the events and the \textit{weights} variable.
We will also explore how we divide our dataset into a training and testing set. Afterwards we will discuss the crucial task of optimizing the architecture and hyperparameters of the ML models for obtaining high-performance classifiers. In this field, the challenge is often to 
distinguish between signal events from rare physics phenomena, such as our DM interactions, and the much more common SM background events. Since the signals events are extremely rare, the classifiers need to be highly optimized in order to effectively separate them from 
the background. In addition, the datasets have numerous features, which can lead to overfitting or poor generalization if not properly optimized. Another hardship is how to mitigate the phenomena of jagged arrays and missing variables described in the previous chapter.\\
\\It is essential to carefully choose the model architecture and hyperparameters, as well as the dataset pre-processing techniques, in order to achieve the best possible performance in the search for the rare DM physics phenomena. This can involve optimizing 
parameters such as the learning rate, the regularization strength. Exclusively for the NNs we have the number of layers and the number of neurons per layer. Exclusively for the BDTs we have the number of trees and the tree depth. The second and third part of this chapter will be just about optimization for the NNs and BDTs respectively.

\clearpage
\section{The datasets}
The way we are making datasets in this project, to keep it as close to model independent as possible, will be of the following format
\begin{itemize}
   \item Make a dataset with all the SM backgrounds and one DM model.
   \item Make a dataset with all the SM backgrounds and all the DM models, divided into different signal regions and statistically combine the results of each signal region
\end{itemize}
The idea behind the first one is to start with a model dependent approach, where we train an ML algorithm to learn just one model at a time. The reason for taking this approach, is because it is still closer to being more model independent than normal ML HEP searches, as these usually train on the whole SM background dataset with just one 
signal model with fixed masses and parameters. While what we would be doing is combining all the different masses and parameters into one.\\
\\For the second one, if we created Signal Regions (SR) in kinematically orthogonal regions, then some models might be more important than other in each region. Meaning that the approach would teach the ML algorithm the signatures of the DM physics in the final state, rather than making an algorithm that is good at learning one individual model.
Furthermore, the plan is to combine the results of each respective region to get an overall view of all of Run II.\\
\\In this project we will look at real and simulated data from the ATLAS detector from all running periods of Run II. This is from period 2015-2016, 2017 and 2018 each with an integrated luminosity of $36.4, 44.3$ and 58.5 $fb^{-1}$ respectively. 
In Table \ref{tab:dataset} we can see the overall number of MC simulated events for each process and expected events (i.e. scaled to the relevant cross-section and integrated luminosity) for all background channels as well as DM models. In addition to using the features in Table \ref{tab:variables}, we will also include the EventID, dataset ID, period, 
dilepton final state, label telling us whether an event is signal or background and the \textit{weight} of each event.
\begin{table}[!h]
   \centering
    \caption[Dataset used for ML]{Table showcasing the number of simulated events and expected events for every background channel and DM model that will be used in this thesis.}
    \footnotesize\begin{tabular}{l|r|r}\midrule\midrule
      Channel                                                                         & Simulated Events            & Expected events for 139 fb$^{-1}$ \\\midrule
      W                                                                               & 38,684                & 5,436.3        \\
      Drell Yan                                                                       & 47,201,697            & 2,198,257.6    \\
      TTbar                                                                           & 28,126,697            & 978,530.8      \\
      Single top                                                                      & 729,624               & 93,898.5       \\
      Diboson                                                                         & 10,983,689            & 116,653.5      \\\midrule
      Standard model total                                                            & 87,080,391            & 3,392,776.7    \\\midrule
      $Z'$ Dark Higgs Heavy Dark Sector                                               & 498,621               & 54.92\footnote{The expected number of events for any BSM model has prefixed assumptions about for example cross-section, decay width, etc. As we do not empirically know if any of these assumptions are correct, these are not set in stone.}         \\
      $Z'$ Dark Higgs Light Dark Sector                                               & 892,365               & 155.37         \\
      $Z'$ Light Vector Heavy Dark Sector                                             & 521,759               & 39.82         \\
      $Z'$ Light Vector Light Dark Sector                                             & 470,352               & 235.70         \\
      $Z'$ Effective Field Theory Heavy Dark Sector                                   & 715,409               & 0.0004           \\
      $Z'$ Effective Field Theory Light Dark Sector                                   & 640,963               & 0.0007          \\
      Supersymmetric direct slepton production                                        & -               & -          \\
      2HDM + a                                                                        & -               & -          \\\midrule\midrule
   \end{tabular}
   \label{tab:dataset}
\end{table}

\subsection{Weights}\label{sec:wgts}
The weights are crucial for most of what is to come further in this thesis. The weights only apply to MC simulations and can be interpreted as corrections to the simulations. The weights on this project are defined as Eq. (\ref{eq:wgts})
\begin{equation}\label{eq:wgts}
   W = w_{mc}\times w_{pu}\times w_{xs}\times w_{lsf} \times w_{nlo,ew} \times w_{ttbar} \times w_{jet} \times w_{bjet} \times \frac{\text{lumi}_{period}}{\sum w_{mc, DSID}}
\end{equation}
This equation has a lot of information in it, so every part of this will be discussed. Starting from the MC weight, $w_{mc}$, which gives the relative probability of an event within a sample \cite{Pythia}, the pile-up weight $w_{pu}$, is a weight used to correct object reconstruction as it can be possible for multiple objects to cross on the 
identification process. \cite{Soyez:2018opl}\\ 
\\The cross-section weight $w_{xs}$, defined as $w_{xs} := xs \times w_{kf} \times g_{eff}$ where $xs$ is the cross-section of the process, $w_{kf}$ is the $k-$factor which tries to correct for the absence of these higher-order term, 
for more details we refer the reader to the study done by Catani et al. \cite{Catani:2001cr}. $g_{eff}$ is the filter efficiency, which is the expected fraction of MC events that pass through the generator criteria.\\
\\The Lepton and trigger Scale Factor (LSF), $w_{lsf}$, which are applied in order to correct for differences in reconstruction in MC with respect to data, the Next to Leading Order (NLO) electroweak correction $w_{nlo,ew}$, which corrects for higher order EW diagrams, 
for more details we refer the reader to the paper by Denner et al. \cite{Denner:2022pwc}. \\
\\The $w_{ttbar}, w_{jet}$ and $w_{bjet}$, which are reconstruction corrections for $t\overline{t}$ \cite{Czakon:2015owf}, jets \cite{ATLAS:2014hvo} and b-jets \cite{ATLAS:2019bwq}, respectively. And lastly the luminosity of the period, meaning
$$
\text{lumi}_{period} = \begin{cases}
               36.4\text{ fb}^{-1},& \text{for period = 2015-2016}\\
               44.3\text{ fb}^{-1},& \text{for  period = 2017}\\
               58.5\text{ fb}^{-1},& \text{for  period = 2018}
               \end{cases}
$$
divided by the Sum Of Weights (SOW), $\sum w_{mc}$ of every sample for each dataset ID.

\subsection{Signal regions}\label{sec:SRs}
As a first step towards achieving less model dependence we will create three Signal Regions (SR). The goal is then to train one network in each respective SR that contains all the DM models we wish to study. The SR we will study are defined below 
\begin{itemize}
   \item SR1: $m_{ll} >110$ GeV and $E_T^{miss} \in [50, 100]$ GeV
   \item SR2: $m_{ll} >110$ GeV and $E_T^{miss} \in [100, 150]$ GeV
   \item SR3: $m_{ll} >110$ GeV and $E_T^{miss} >150$ GeV
\end{itemize}
where the $m_{ll}$ cut is made such that we remove the $Z-$peak before training and where the MET regions are orthogonal. 
The goal of having kinematically orthogonal SRs when training is to allow the combination of the results.



\subsection{Train and test split}\label{sec:train_test}
When training our networks we will use 80\% of the dataset, the remaining 20\% will be used to test whether the networks are learning any physical patterns or learning the dataset. When splitting the datasets however, we need to be absolutely certain that the distributions 
follow the same shape and have the same ratio of background channels as well as having good MC and data agreement. In this thesis we will utilize a function from \verb|scikit learn| \cite{scikit-learn} called \verb|train_test_split| which can easily split our dataset into 
a training and testing set of a chosen size, while also sorting the label of the event. Another benefit of using this function is that is has included a random seed such that we can always split the datasets in the same shape.
\graphicspath{{../../../Plots/Data_Analysis/train_test_split/}}
\begin{figure}[!ht]
	\centering
	\begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{mt2_train.pdf}
        \caption{Training distribution}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{mt2_test.pdf}
        \caption{Testing distribution}
     \end{subfigure}
	\caption[Train-test split distribution]{Train and test distribution of the stransverse mass when dividing SM background and DM samples into 80\% and 20\% datasets. The integrated luminosity for each distribution is 80\% and 20\% of 139 $fb^{-1}$ respectively. We also included an estimated 20\% flat systematic uncertainty to the simulated events.}\label{fig:train-test}
\end{figure}
\\In Figure \ref{fig:train-test} we can see that the distribution remains the same when we split the dataset into 80-20, where we also see the data and MC agree. To see the distribution of every other kinematical variable see the GitHub repo\footnote{Available here: \href{https://github.com/rubenguevara/Master-Thesis/tree/master/Plots/Data_Analysis/train_test_split}{https://github.com/rubenguevara/Master-Thesis/tree/master/Plots/\\Data\_Analysis/train\_test\_split}}.


\clearpage
\section{Neural Network Training}\label{chap:NN_train}
For this thesis we utilize \verb|TensorFlow v. 2.7.1 GPU| for all NNs. After creating the dataset, the first and most important thing is to marinate the \verb|batch_size| whenever we try anything while using the dataset. This is to optimize wrt. to both the size 
of the dataset and the imbalance between signal and background events.\\
\\The highest possible batch size that could be used for this thesis was $2^{22}$,\footnote{It has to be a power of two because of the alignment of virtual processors (VP) onto physical processors (PP) of the GPU. As the number of PP is often a power of 2, using a number of VP different from a power of 2 leads to poor performance.} 
leading to roughly 4 million samples per batch. This is the best that a dedicated GPU, \verb|NVIDIA A100-PCIE-40GB|, could handle. The batch size also decreases the more complex the NN becomes, as this requires greater computational power.\\
\\With this out of the way there are still a few challenges to mitigate to optimize our NN. These are described below.

\subsection{Padding of data}\label{sec:padding_NN}
There are two difficulties to overcome when utilizing NNs as compared to BDTs. The hardest to solve, as there is no general solution yet, is the padding of \textit{jagged arrays}. As indicated in Table \ref{tab:variables} with the $^\dagger$, we will record events 
with up to three jets in the final state. However, there isn't always three jets in the recorded events that we will be studying, this creates jagged arrays which we can interpret as arrays with missing values. This is an unwanted feature that we need to avoid when 
training NNs.\\
\\To mediate this problem we chose to set the $p_T$ to zero for the missing jets and $m_{jj}$ to zero if there are less than two jets, this is something that is physically reasonable as it doesn't violate any conservation laws. 
More problematic however is the $\eta$ and $\phi$ when there aren't jets. To mediate this we have set the values to -999, which has no physical meaning and is impossible to achieve, this we did so it becomes easier for us to identify the jagged arrays that 
need different interventions.\\
\\As mentioned before, there is no consensus on how the padding should be done, and there are many methods of doing so. The classical data scientist way of solving this problem would be to just take 
the mean of every feature and use that as a variable for every event with missing values. That means replacing every $p_T, m_{jj} = 0$ and $\eta,\phi=-999$ with the mean of every $p_T$, $m_{jj}$, $\eta$ and $\phi$ (excluding the 0's and -999's respectively). 
However, this is not popular among physicists since it breaks conservation laws when we say there are jets present in an event when in reality there are none. Another approach is to use Bayesian statistics or ML to estimate the missing values, 
these options will not be pursued in this thesis due to time, but might be of interest for future projects. Another approach, is setting all the missing values to zero, as this might mean that there isn't anything there, 
but this also breaks conservation laws since $\eta,\phi=0$ have physical meaning, this is also highly looked down upon by data scientists since this could affect the weighting when training the network and potentially create a false pattern for the network to follow which would lead to a bias.\\ 
\\The jet $p_T$ and $m_{jj}$ being 0 is a valid form of padding the dataset, as this doesn't break any fundamental law of physics. However, setting $\phi$ as something outside $[-\pi,\pi]$ doesn't make much sense 
as this is the angle around the detector. Setting a high value of $\abs{\eta}$ might possible in principle, but as of today the ATLAS detector has a $\abs{\eta}<4.9$ (see Chapter \ref{sec:ATLAS}) as the pseudorapidity 
states how close to the beam line the objects recorded are. However, having a $p_T$ of a jet equal to zero while still recording the $\eta$ and $\phi$ breaks the laws of physics, so this is a problem that needs to be fixed.\\
\\Another approach is to remove the features with missing values to conserve statistics, albeit make it harder for the network to see any pattern that we might miss, but this is also not a desirable mitigation. 
Instead, we have tried to create new kinematic variables that work around the need of padding. These features are just \textit{event features}, meaning that we count the number of jets that fulfill some criteria, 
such as the number of b-jets with $p_T > 20$ GeV, the number of light jets with $p_T>40$ GeV, the number of jets recorded in the central calorimeter ($\abs{\eta} < 2.5$), and the number of 
jets with $p_T>50$ GeV recorded in the forward calorimeter ($\abs{\eta} > 2.5$). The $p_T$ cuts are optimized to allow a good agreement between data and simulations, the full distributions with different cuts in the preselection region can be seen in Appendix \ref{appendix:no_padding}\todo{To come}. 
The variables included in the training are shown in Table \ref{tab:padding_variables}.\\
\begin{table}[!h]
   \centering
   \caption[New features that need no padding]{Table showcasing features that will not need padding.}
   \begin{tabular}{l|r}\midrule\midrule
      Kinematic variable                                                      & Feature name          \\\midrule
      Number of b-jets with $p_T > 20$ GeV                                    & n\_bjetPt20\\
      Number of light-jets with $p_T > 40$ GeV                                & n\_ljetPt40\\
      Number of jets recorded in Central calorimeter                          &n\_jetsetaCentral\\
      Number of jets recorded in Forward calorimeter with $p_T > 50$ GeV      & n\_jetsetaForward50\\\midrule\midrule
   \end{tabular}
   \label{tab:padding_variables}
\end{table}
\\When training our NN with these new variables instead of dropping features with missing variables we hope that the NN learns more physics by hopefully recognizing patters between all high level features. 

\subsection{Normalization of data}\label{sec:normie_NN}
Since neural networks send a lot of data into multiple neurons and multiple layers using activation functions and carrying weights and biases that change 
for every back propagation iteration, it is important to make sure that a neuron output doesn't die when moving around the network. Meaning that a neuron output becomes insignificant compared to others when navigating the loss-phase space.
A fast way for neurons to die off is to not normalize the data and send it through the network as it is available. The reason it might die is because we send in numbers which vary significantly to each other, i.e. the $p_{T}$ might be as high as thousands GeV, while 
$E_T^{miss}/\sigma$ might be as low as 0.1. What might happen when sending such different numbers is that the network might think "obviously the high number is more important than the low number" thus making the activation function worse for the feature, even though 
this feature is of high importance when looking at MET final states. A way to fix this problem is to normalize all features. There are many ways to do this, one could do \textit{min max scaling} which normalizes every feature from $[0,1]$, completely solving the problem above. 
Mathematically speaking this is done by
\begin{equation}\label{eq:minmax}
   X_{norm} = \frac{X - X_{min}}{X_{max}-X_{min}}
\end{equation}
Where $X$ is the array containing all events for a given feature, while $X_{min}$ and $X_{max}$ are the lowest and highest values in the said array. Another way to normalize the data is to make the mean of every feature such that the mean of all the features is distributed according 
to a standard normal distribution (mean 0 and std. 1), this is called \textit{Z-score normalization}
\begin{equation}\label{eq:Z-score}
   X_{norm} = \frac{X - \bar{X}}{\sqrt{\sigma_X^2}}
\end{equation}
where $\bar{X}$ is the mean of said array and $\sigma_X^2$ is the variance. One could also use pre-built functions in \verb|TensorFlow| that provide a normalization, such as \verb|Batch_normalization| which normalizes the data entering the network per batch. This is usually used 
in Convolutional NN's as it improves computational speed. Another one, \verb|Normalize|, provides the same as Eq. (\ref{eq:Z-score}) for the whole training set going in. This is however computationally heavy to use. The \verb|Layer_normalization|, which normalizes the activations 
of the previous layer in a batch \textit{independently}, rather than \textit{across} a batch like \verb|Batch_Normalization|. Both \verb|Batch_Normalization| and \verb|Layer_Normalization| use an optimized version of the Z-score when normalizing the data, meaning that all the features 
take are normalized following a standard normal distribution.\\
\\There is a big difference when normalizing data ourselves or using \verb|TensorFlow|. \verb|TensorFlow| remembers how the data was normalized when training such that the test data will be normalized the same way, making testing easier. While if we use Eq. (\ref{eq:minmax}) or 
Eq. (\ref{eq:Z-score}) ourselves, we have to make sure that we use the same values for $X_{max}, X_{min}, \bar{X}$ or $\sigma_X^2$ when normalizing the test data. 



\subsection{Balancing of signal and background}\label{sec:balance_NN}
The biggest challenge to overcome in this thesis is what we should use as sample weights (see Section \ref{sec:sample_weight}). If we were to not use any form of sample weights to mitigate the imbalance in our data set it could potentially lead to \textit{majority class classification} where the 
networks could get "lazy" and guess that everything is background. \\
\\To combat the majority class classification, we will as mentioned make use of the sample weights. We will study three cases
\begin{enumerate}
   \item Unweighted training, meaning that we will be setting the sample weight to one
   \item Weighted training, meaning that we will be setting the sample weight to the weights used to re-weight MC events to expected events explained in Chapter \ref{sec:wgts}
   \item Balanced training, we will weigh the background and signal samples in an attempt to balance the number of events available in each of the two categories
   % where we will make set the weight of all signal events to one, while weighing down the background by the ratio of signal events divided by background events, $\frac{N_{sig}}{N_{bkg}}$ 
\end{enumerate}
Where the latter would in terms of weights mimic a 50-50 distribution of signal and background. 
% To test the different balancing methods we will use a dataset containing only SM processes from Table \ref{tab:dataset}, and train a network to single our the $W$ channel from the other SM backgrounds as this has the lowest statistics. 
% Meaning that it more accurately mimics our low statistics DM signal from Table \ref{tab:dataset} that gives rise to the dataset imbalance.

\subsection{Re-weighting MC to expected events}\label{sec:sample_wgts_NN}
Even if the weighting method previously described might help the NN give us better results, we also want to include the weights used to re-weight MC events to expected events. This is desirable in the sense that we want to show our ML networks the true kinematical distributions of each feature.
As the re-weighting weights are generated with simulation corrections as well as luminosity and cross-section in mind, it is heavily desirable to also apply these corrections when training our networks, such that it can correctly predict new events regardless of their weight. This is particularly 
crucial when using real data on our predictions, as these have no weights.\\
\\Ideally one would take into account both the data imbalance between the signal and background as well as the re-weighting weights when training a network. To do this using \verb|TensorFlow| we could make use of two parameters when training the network: \verb|class_weight| and \verb|sample_weight|. 
\verb|class_weight| works as a dictionary that weighs events differently that based on a dictionary key, for our purposes this key would be whether the event is a signal or background event. 
\verb|sample_weight| takes in individual weights for every single event that goes into the network, meaning that it is crucial that we know that the desired weight matches the desired event. Ideally we would use both weighting methods, \verb|class_weight| to balance the signal to 
background ratio and \verb|sample_weight| with the re-weighting weights that include the cross-section. However, there is a bug in \verb|TensorFlow| (up to version \verb|GPU 2.7.1|) that makes it, so the program doesn't run when using both parameters. This is not a big problem though, as when looking at the source code \cite{Keras_source_code} 
one can see that what \verb|TensorFlow| does with both weights is multiply them together. \\
\\For this thesis we tried testing four different methods to re-weight and balance the dataset. Firstly all background events will be re-weighted to expected events %\footnote{The reason we do not re-weight the signal events is both because we do not actually know what the cross-section is, and because the cross-section is so low compared to the SM that it would punish the ML algorithm}, 
and in addition we expanded the balancing ratio from Chapter \ref{sec:balance_NN} to be
\begin{enumerate}
   \item $\frac{N_{sig,MC}}{N_{bkg,MC}}$ where we weigh down all background events wrt. the number of simulated events
   \item $\frac{N_{bkg,MC}}{N_{sig,MC}}$ where we weigh up all signal events wrt. the number of simulated events
   \item $\frac{N_{sig,MC}}{N_{bkg,exp}}$ where we weigh down all background events wrt. the number of expected events
   \item $\frac{N_{bkg,exp}}{N_{sig,MC}}$ where we weigh up all signal events wrt. the number of expected events
\end{enumerate}
Note that we are not re-weighting the signal events to expected signal events when training because this would in principle remove all expected events (due small cross-sections) from the NN as there are so few events (see Table \ref{tab:dataset}). 
Taken all of these factors into account what remains is to choose a network architecture and which hyperparameters to use to best fit our task.


\clearpage
\subsection{Architecture and hyperparameter tuning}\label{sec:NNgriddy}
The architecture of the NN utilized in this project is of the form shown in Figure \ref{fig:FFN}. An algorithm showing one of the possibilities can be seen in Algorithm \ref{alg:nn}. To get the best performance on our NN, we need to find which hyperparameters help the network reach the highest significance. To do this, we need to do a grid search. 
For our neural network we will mainly focus on four hyperparameters explained in section \ref{sec:theory_nn}:
\begin{itemize}
   \item The learning rate $\eta$
   \item The L2-regressor variable $\lambda$
   \item The number of neurons on each hidden layer \verb|n_neuron|
   \item The number of layers \verb|n_layers|, excluding the output. Meaning that \verb|n_layers| = 1 means no hidden layer
\end{itemize}
The metrics that will be used to estimate the best hyperparameters are AUC, binary accuracy and most importantly the expected significance\footnote{See Chapter \ref{sec:tools} for explanation of metrics.}.
The expected significance for this section has been calculated using the low statistics formula Eq. (\ref{eq:exp_sig}) without uncertainties. The expected significance will be calculated by making a cut on 0.85 on the network prediction score, as explained in Chapter \ref{sec:siggy}. 
For some models where the expected events are too low, we will get that the expected significance is 0 or \verb|NaN|. If this happens we will use the AUC on the testing set as a metric to find the best hyperparameters. 
This will procedure will be done for every single network we will explore.
\clearpage


\section{Boosted Decision Tree Training}
When working with BDTs there are not as many challenges to overcome as with NNs. For example the padding and normalization of data can be completely avoided, making the whole procedure a lot easier when one uses challenging dataset as we do in high energy physics.
The weights are still an obstacle that needs to overcome when using BDTs. This will be discussed in the next section.\\
\\For this project we will as mentioned utilize the eXtreme Gradient Boosting, or \verb|XGBoost| for short, package \cite{XGBoost} made for the Higgs ML Challenge \cite{HiggsChallenge} whenever we mention BDTs.
This project utilized version \verb|1.5.0| without GPU adaptability. \verb|XGBoost| also helps to avoid padding as it is integrated with a \verb|missing_variable| variable where we can simply write the number of the variable that is missing.

\subsection{Sample weights}\label{sec:bdt_wgts}
For \verb|XGBoost| there is a different problem when it comes to weights. XGBoost has a variable called \verb|scale_pos_weight| where we can help the network deal with imbalanced data, such as the one we have. 
Meaning that the whole problem of combining the re-weighting weights with the balancing weights from Chapter \ref{sec:sample_wgts_NN} completely disappears. Albeit there is a caveat, \verb|XGBoost| does not have the possibility to include negative weights, which the datasets explored in this work have. The reason \verb|XGBoost| 
does not include negative weights, is because when calculating the number of events in a leaf node, which is made by taking the sum of sample weights, we cannot have a negative value \cite{neg_wgt_xbg1, neg_wgt_xbg2}.
As for the MC generators, Sherpa \cite{Sherpa} takes into account higher order diagrams and needs to add negative weights to "counter" the over counting of diagrams \cite{Negative_Weights_article}, which are important to correctly scale the simulated events to real data.\\
\\A method to mitigate this problem is to use the absolute value of the weights when training. This is however not generally accepted as a solution, and some even say it should be avoided. There are other options however, one of these options is to not include events with negative weights on the training set. 
This is an okay thing to do, as we can imagine that if we were to only include events with positive weights on the training, it might be the same as putting the negative weights on the "testing dataset" (Chapter \ref{sec:train_test}). 
However, both methods equally problematic if the positive and negative weights distributions are not equal for all the features.\\
\\Another method that has been used on a published ATLAS article \cite{Abbott:2714377} is to normalize the weights when using the absolute value with respect to the sum of weights over the sum of absolute weights. The reason behind this is that the sum of weights is obviously not the same when we take the absolute value. 
Mathematically speaking, if we have an array of weights $W$, we can update this like
\begin{equation}\label{eq:ATLAS_wgt}
   W \rightarrow \abs{W}\frac{\sum_i W_i}{\sum_i\abs{W_i}}
\end{equation}
such that the weights are at least on the same scale. 

\subsection{Architecture and hyperparameter tuning}\label{sec:BDTGriddy}
Making a BDT for our purposes is fairly easy as well using XGBoost. One way to do it is using the algorithm shown in Algorithm \ref{alg:xgb}. 
To get the best performance on our BDT we have to do a grid search here as well. The trainable hyperparameters here are different from NN's, with XGBoost we will focus on the following hyperparameters
\begin{itemize}
   \item Tree depth: how many times we split the data
   \item Number of estimators: how many trees we use to do the gradient boosting
   \item The learning rate $\eta$
   \item L2-regressor $\lambda$, to stop overtraining
\end{itemize}
The same procedure for hyperparameter optimizations as described above for NNs will be used for BDTs as well.

\newpage
\section{Results}
We tested all the methods explained in this chapter, the full results can be seen in Appendix \ref{chap:network_opt}, in this part we will highlight the main results. We observed that for the NNs using \verb|Batch_normalization| as the normalization method yielded best results, using ADAM as the optimizer yielded far better results than using SGD. 
As sample weights we saw that the method that yielded the best results was the one that re-weighted every background event and balanced the dataset by weighing up all signal events by the ratio of expected number of background events over signal MC events, $\frac{N_{bkg,exp}}{N_{sig,MC}}$. We also observed that the new padding method presented 
performed better than the other methods. \\
\\For the BDT we observed that while conducting a grid search for the optimal hyperparameters, we never hit a plateau where the scores seemed to flat out when increasing the tree depth. However, we decided to take the conservative approach of having a tree depth of maximum 6. The sample weights we decided to use further with BDTs were the ones 
that only looked at the positive weights of the dataset. \\
\\The best results from the BDT and NN trained using the model dependent approach on the Dark Higgs Heavy Dark Sector model can be seen in Figure \ref{fig:BDT_vs_NN}.
\newpage
\begin{figure}[!ht]
	\centering
   \graphicspath{{../../../Plots/XGBoost/Weighting_methods/}}
   \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{Pos_wgt/VAL.pdf}
      \caption{BDT result}
   \end{subfigure}
   \graphicspath{{../../../Plots/NeuralNetwork/Padding/}}
   \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{New_pad/VAL.pdf}
      \caption{NN result}
   \end{subfigure}
   \graphicspath{{../../../Plots/XGBoost/Weighting_methods/}}
   \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{Pos_wgt/EXP_SIG.pdf}
      \caption{Significance of a)}
   \end{subfigure}
   \graphicspath{{../../../Plots/NeuralNetwork/Padding/}}
   \begin{subfigure}[b]{0.49\textwidth}
      \centering
      \includegraphics[width=1\textwidth]{New_pad/EXP_SIG.pdf}
      \caption{Significance of b)}
   \end{subfigure}
   \caption[Comparison of BDT and NN]{Comparison of BDT and NN. Both networks were tested on the DH HDM dataset using the model dependent approach}\label{fig:BDT_vs_NN}
\end{figure}
\noindent From these results we can see that the BDTs perform better than NNs. As using BDTs is easier than NNs, as well as BDTs having greater interpretability, we will for the remainder of the thesis opt to only use BDTs as our main ML algorithm. We have now explored the methods that will be used for this search, and all the challenges that need to be mitigated. The next part of this thesis will present 
the results of both the model dependent and independent approach for every model. 




\end{document}